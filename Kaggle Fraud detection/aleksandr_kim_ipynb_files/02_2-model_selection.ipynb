{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_features = pd.read_csv(folder_path + \"top_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_features = sorted(top_features.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C2',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'D1',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'DeviceInfo',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'P_emaildomain',\n",
       " 'P_emaildomain_2',\n",
       " 'ProductCD',\n",
       " 'R_emaildomain',\n",
       " 'TransactionAmt',\n",
       " 'TransactionAmt_to_mean_card1',\n",
       " 'TransactionAmt_to_mean_card2',\n",
       " 'TransactionAmt_to_mean_card3',\n",
       " 'TransactionAmt_to_mean_card4',\n",
       " 'TransactionAmt_to_mean_card5',\n",
       " 'TransactionAmt_to_mean_card6',\n",
       " 'TransactionAmt_to_std_card1',\n",
       " 'TransactionAmt_to_std_card2',\n",
       " 'TransactionAmt_to_std_card3',\n",
       " 'TransactionAmt_to_std_card4',\n",
       " 'TransactionAmt_to_std_card5',\n",
       " 'TransactionAmt_to_std_card6',\n",
       " 'V126',\n",
       " 'V127',\n",
       " 'V128',\n",
       " 'V13',\n",
       " 'V130',\n",
       " 'V131',\n",
       " 'V20',\n",
       " 'V282',\n",
       " 'V283',\n",
       " 'V285',\n",
       " 'V291',\n",
       " 'V294',\n",
       " 'V306',\n",
       " 'V307',\n",
       " 'V308',\n",
       " 'V310',\n",
       " 'V312',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V315',\n",
       " 'V317',\n",
       " 'V35',\n",
       " 'V38',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V56',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V78',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V87',\n",
       " 'addr1',\n",
       " 'card1',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card4',\n",
       " 'card5',\n",
       " 'card6',\n",
       " 'dist1',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_13',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_30',\n",
       " 'id_31',\n",
       " 'id_33']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_test = pd.read_csv(folder_path + \"X_test.csv\")\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = pd.read_csv(folder_path + \"X.csv\")\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target_train = pd.read_csv(folder_path + \"target_train.csv\")\n",
    "# target_train = target_train.isFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical = [\n",
    "    # \"DeviceInfo\", too much for ohe\n",
    "    \"DeviceType\",\n",
    "    \"M1\",\n",
    "    \"M2\",\n",
    "    \"M3\",\n",
    "    \"M4\",\n",
    "    \"M5\",\n",
    "    \"M6\",\n",
    "    \"M7\",\n",
    "    \"M8\",\n",
    "    \"M9\",\n",
    "    \"P_emaildomain\",\n",
    "    \"P_emaildomain_1\",\n",
    "    \"P_emaildomain_2\",\n",
    "    \"P_emaildomain_3\",\n",
    "    \"ProductCD\",\n",
    "    \"R_emaildomain\",\n",
    "    \"R_emaildomain_1\",\n",
    "    \"R_emaildomain_2\",\n",
    "    \"R_emaildomain_3\",\n",
    "    \"card4\",\n",
    "    \"card6\",\n",
    "    \"id_12\",\n",
    "    # \"id_13\",\n",
    "    # \"id_14\",\n",
    "    \"id_15\",\n",
    "    \"id_16\",\n",
    "    # \"id_17\",\n",
    "    # \"id_19\",\n",
    "    # \"id_20\",\n",
    "    \"id_28\",\n",
    "    \"id_29\",\n",
    "    # \"id_30\",\n",
    "    # \"id_31\",\n",
    "    \"id_32\",\n",
    "    # \"id_33\",\n",
    "    \"id_34\",\n",
    "    \"id_35\",\n",
    "    \"id_36\",\n",
    "    \"id_37\",\n",
    "    \"id_38\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_top_features = sorted(list(set(top_features) & set(categorical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'P_emaildomain',\n",
       " 'P_emaildomain_2',\n",
       " 'ProductCD',\n",
       " 'R_emaildomain',\n",
       " 'card4',\n",
       " 'card6']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_top_features = sorted(list(set(top_features) - set(categorical_top_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numeric_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: 8 columns of id will not be OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upload data_train_downsample_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train_downsample_top_features = pd.read_csv(\n",
    "    folder_path + \"data_train_downsample_top_features.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C2</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>D1</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>DeviceInfo</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>TransactionAmt_to_mean_card1</th>\n",
       "      <th>TransactionAmt_to_mean_card2</th>\n",
       "      <th>TransactionAmt_to_mean_card3</th>\n",
       "      <th>TransactionAmt_to_mean_card4</th>\n",
       "      <th>TransactionAmt_to_mean_card5</th>\n",
       "      <th>TransactionAmt_to_mean_card6</th>\n",
       "      <th>TransactionAmt_to_std_card1</th>\n",
       "      <th>TransactionAmt_to_std_card2</th>\n",
       "      <th>TransactionAmt_to_std_card3</th>\n",
       "      <th>TransactionAmt_to_std_card4</th>\n",
       "      <th>TransactionAmt_to_std_card5</th>\n",
       "      <th>TransactionAmt_to_std_card6</th>\n",
       "      <th>V126</th>\n",
       "      <th>V127</th>\n",
       "      <th>V128</th>\n",
       "      <th>V13</th>\n",
       "      <th>V130</th>\n",
       "      <th>V131</th>\n",
       "      <th>V20</th>\n",
       "      <th>V282</th>\n",
       "      <th>V283</th>\n",
       "      <th>V285</th>\n",
       "      <th>V291</th>\n",
       "      <th>V294</th>\n",
       "      <th>V306</th>\n",
       "      <th>V307</th>\n",
       "      <th>V308</th>\n",
       "      <th>V310</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V317</th>\n",
       "      <th>V35</th>\n",
       "      <th>V38</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V53</th>\n",
       "      <th>V54</th>\n",
       "      <th>V56</th>\n",
       "      <th>V61</th>\n",
       "      <th>V62</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V78</th>\n",
       "      <th>V82</th>\n",
       "      <th>V83</th>\n",
       "      <th>V87</th>\n",
       "      <th>addr1</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>dist1</th>\n",
       "      <th>id_01</th>\n",
       "      <th>id_02</th>\n",
       "      <th>id_05</th>\n",
       "      <th>id_06</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_30</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_33</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>P_emaildomain_2</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>card4</th>\n",
       "      <th>card6</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>68.500</td>\n",
       "      <td>0.194640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467667</td>\n",
       "      <td>0.257761</td>\n",
       "      <td>0.357614</td>\n",
       "      <td>0.360461</td>\n",
       "      <td>0.184566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273001</td>\n",
       "      <td>0.170233</td>\n",
       "      <td>0.205289</td>\n",
       "      <td>0.202121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.542594</td>\n",
       "      <td>1.1624</td>\n",
       "      <td>1.083891</td>\n",
       "      <td>1.120779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>315.0</td>\n",
       "      <td>13926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>49.000</td>\n",
       "      <td>0.365477</td>\n",
       "      <td>0.389806</td>\n",
       "      <td>0.334536</td>\n",
       "      <td>0.367973</td>\n",
       "      <td>0.347265</td>\n",
       "      <td>0.421206</td>\n",
       "      <td>0.491192</td>\n",
       "      <td>0.237929</td>\n",
       "      <td>0.195285</td>\n",
       "      <td>0.214723</td>\n",
       "      <td>0.208132</td>\n",
       "      <td>0.257389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>272.0</td>\n",
       "      <td>5937</td>\n",
       "      <td>555.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2526</td>\n",
       "      <td>75.887</td>\n",
       "      <td>1.289521</td>\n",
       "      <td>1.670997</td>\n",
       "      <td>1.412687</td>\n",
       "      <td>0.573218</td>\n",
       "      <td>0.839154</td>\n",
       "      <td>0.399333</td>\n",
       "      <td>2.636139</td>\n",
       "      <td>2.273508</td>\n",
       "      <td>1.261766</td>\n",
       "      <td>0.298868</td>\n",
       "      <td>0.374466</td>\n",
       "      <td>0.223918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.215393</td>\n",
       "      <td>166.215393</td>\n",
       "      <td>166.215393</td>\n",
       "      <td>90.327904</td>\n",
       "      <td>90.327904</td>\n",
       "      <td>90.327904</td>\n",
       "      <td>90.327904</td>\n",
       "      <td>90.327904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16496</td>\n",
       "      <td>352.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>191631.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "      <td>307</td>\n",
       "      <td>42</td>\n",
       "      <td>86</td>\n",
       "      <td>46</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>57.950</td>\n",
       "      <td>0.868009</td>\n",
       "      <td>0.461005</td>\n",
       "      <td>0.395640</td>\n",
       "      <td>0.435185</td>\n",
       "      <td>0.410694</td>\n",
       "      <td>0.498140</td>\n",
       "      <td>3.931656</td>\n",
       "      <td>0.281387</td>\n",
       "      <td>0.230955</td>\n",
       "      <td>0.253943</td>\n",
       "      <td>0.246148</td>\n",
       "      <td>0.304402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542594</td>\n",
       "      <td>1.1624</td>\n",
       "      <td>1.083891</td>\n",
       "      <td>1.120779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>315.0</td>\n",
       "      <td>7055</td>\n",
       "      <td>555.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>39.000</td>\n",
       "      <td>0.234679</td>\n",
       "      <td>0.271014</td>\n",
       "      <td>0.266263</td>\n",
       "      <td>0.294589</td>\n",
       "      <td>0.347342</td>\n",
       "      <td>0.335245</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.204789</td>\n",
       "      <td>0.155431</td>\n",
       "      <td>0.153595</td>\n",
       "      <td>0.198215</td>\n",
       "      <td>0.204861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>299.0</td>\n",
       "      <td>7875</td>\n",
       "      <td>314.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134634</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>54.500</td>\n",
       "      <td>0.629797</td>\n",
       "      <td>0.629797</td>\n",
       "      <td>0.372086</td>\n",
       "      <td>0.409277</td>\n",
       "      <td>0.557002</td>\n",
       "      <td>0.468484</td>\n",
       "      <td>0.506516</td>\n",
       "      <td>0.506516</td>\n",
       "      <td>0.217205</td>\n",
       "      <td>0.238825</td>\n",
       "      <td>0.402796</td>\n",
       "      <td>0.286280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>325.0</td>\n",
       "      <td>3166</td>\n",
       "      <td>559.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134635</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "      <td>250.000</td>\n",
       "      <td>1.222648</td>\n",
       "      <td>1.925797</td>\n",
       "      <td>1.706816</td>\n",
       "      <td>1.877415</td>\n",
       "      <td>1.771762</td>\n",
       "      <td>1.315552</td>\n",
       "      <td>0.712097</td>\n",
       "      <td>1.137362</td>\n",
       "      <td>0.996354</td>\n",
       "      <td>1.095527</td>\n",
       "      <td>1.061899</td>\n",
       "      <td>0.737669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.599166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847843</td>\n",
       "      <td>0.817171</td>\n",
       "      <td>0.991114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.351473</td>\n",
       "      <td>43.319174</td>\n",
       "      <td>26.806977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542594</td>\n",
       "      <td>1.1624</td>\n",
       "      <td>1.083891</td>\n",
       "      <td>1.120779</td>\n",
       "      <td>0.577586</td>\n",
       "      <td>0.619982</td>\n",
       "      <td>1.120979</td>\n",
       "      <td>0.829785</td>\n",
       "      <td>0.867563</td>\n",
       "      <td>0.544278</td>\n",
       "      <td>0.587557</td>\n",
       "      <td>1.144462</td>\n",
       "      <td>0.84461</td>\n",
       "      <td>0.881965</td>\n",
       "      <td>1.099456</td>\n",
       "      <td>272.0</td>\n",
       "      <td>1214</td>\n",
       "      <td>174.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>172059.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>17</td>\n",
       "      <td>249</td>\n",
       "      <td>226</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>449</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134636</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>279.950</td>\n",
       "      <td>1.333976</td>\n",
       "      <td>1.402013</td>\n",
       "      <td>1.911292</td>\n",
       "      <td>2.114622</td>\n",
       "      <td>1.306741</td>\n",
       "      <td>1.473155</td>\n",
       "      <td>0.899892</td>\n",
       "      <td>0.907036</td>\n",
       "      <td>1.115718</td>\n",
       "      <td>1.102536</td>\n",
       "      <td>0.647020</td>\n",
       "      <td>0.826042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>299.0</td>\n",
       "      <td>15066</td>\n",
       "      <td>170.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134637</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>107.950</td>\n",
       "      <td>0.119430</td>\n",
       "      <td>0.814468</td>\n",
       "      <td>0.737003</td>\n",
       "      <td>0.810668</td>\n",
       "      <td>0.765047</td>\n",
       "      <td>0.927942</td>\n",
       "      <td>0.133132</td>\n",
       "      <td>0.498286</td>\n",
       "      <td>0.430226</td>\n",
       "      <td>0.473049</td>\n",
       "      <td>0.458528</td>\n",
       "      <td>0.567045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>209.949997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>485.0</td>\n",
       "      <td>13071</td>\n",
       "      <td>321.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134638</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>117.000</td>\n",
       "      <td>0.953051</td>\n",
       "      <td>0.781764</td>\n",
       "      <td>0.798790</td>\n",
       "      <td>0.883768</td>\n",
       "      <td>1.042027</td>\n",
       "      <td>1.005736</td>\n",
       "      <td>0.662777</td>\n",
       "      <td>0.519278</td>\n",
       "      <td>0.466294</td>\n",
       "      <td>0.460785</td>\n",
       "      <td>0.594644</td>\n",
       "      <td>0.614583</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1035.500000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>2903.500000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>669.500000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>317.500000</td>\n",
       "      <td>669.500000</td>\n",
       "      <td>317.500000</td>\n",
       "      <td>2234.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>387.0</td>\n",
       "      <td>7826</td>\n",
       "      <td>481.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134639 rows  101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         C1  C10  C11  C12   C13  C14   C2   C5   C6   C8   C9     D1    D10  \\\n",
       "0       1.0  0.0  2.0  0.0   1.0  1.0  1.0  0.0  1.0  0.0  1.0   14.0   13.0   \n",
       "1       1.0  0.0  1.0  0.0   1.0  1.0  1.0  0.0  1.0  0.0  1.0    0.0    0.0   \n",
       "2       1.0  1.0  2.0  2.0   2.0  1.0  4.0  0.0  1.0  1.0  0.0    1.0    0.0   \n",
       "3       4.0  0.0  3.0  0.0  22.0  3.0  4.0  1.0  5.0  0.0  2.0    0.0  465.0   \n",
       "4       1.0  0.0  1.0  0.0   1.0  1.0  1.0  0.0  1.0  0.0  1.0    0.0    0.0   \n",
       "...     ...  ...  ...  ...   ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "134634  2.0  0.0  1.0  0.0   7.0  2.0  1.0  0.0  1.0  0.0  2.0  309.0  309.0   \n",
       "134635  1.0  1.0  1.0  0.0   1.0  1.0  1.0  0.0  1.0  1.0  0.0    0.0    0.0   \n",
       "134636  2.0  0.0  1.0  0.0   1.0  1.0  1.0  1.0  1.0  0.0  2.0    0.0    1.0   \n",
       "134637  1.0  0.0  1.0  0.0   2.0  1.0  1.0  1.0  1.0  0.0  1.0    6.0    6.0   \n",
       "134638  1.0  0.0  1.0  1.0   5.0  1.0  1.0  0.0  3.0  0.0  2.0   22.0   22.0   \n",
       "\n",
       "          D11  D12  D13  D14    D15     D2    D3     D4    D5   D6    D8   D9  \\\n",
       "0        13.0  0.0  0.0  0.0    0.0    0.0  13.0    0.0   0.0  0.0   0.0  0.0   \n",
       "1         0.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0   \n",
       "2         0.0  0.0  0.0  0.0    0.0    1.0   0.0    0.0   0.0  0.0  83.0  0.0   \n",
       "3       423.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0   \n",
       "4         0.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0   \n",
       "...       ...  ...  ...  ...    ...    ...   ...    ...   ...  ...   ...  ...   \n",
       "134634  448.0  0.0  0.0  0.0  309.0  309.0  32.0  448.0  32.0  0.0   0.0  0.0   \n",
       "134635    0.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0   \n",
       "134636    0.0  0.0  0.0  0.0    1.0    0.0   0.0    1.0   1.0  0.0   0.0  0.0   \n",
       "134637    6.0  0.0  0.0  0.0    6.0    6.0   6.0    6.0   6.0  0.0   0.0  0.0   \n",
       "134638   22.0  0.0  0.0  0.0   22.0   22.0   0.0   22.0   0.0  0.0   0.0  0.0   \n",
       "\n",
       "        DeviceInfo  TransactionAmt  TransactionAmt_to_mean_card1  \\\n",
       "0             2740          68.500                      0.194640   \n",
       "1             2740          49.000                      0.365477   \n",
       "2             2526          75.887                      1.289521   \n",
       "3             2740          57.950                      0.868009   \n",
       "4             2740          39.000                      0.234679   \n",
       "...            ...             ...                           ...   \n",
       "134634        2740          54.500                      0.629797   \n",
       "134635         116         250.000                      1.222648   \n",
       "134636        2740         279.950                      1.333976   \n",
       "134637        2740         107.950                      0.119430   \n",
       "134638        2740         117.000                      0.953051   \n",
       "\n",
       "        TransactionAmt_to_mean_card2  TransactionAmt_to_mean_card3  \\\n",
       "0                           0.000000                      0.467667   \n",
       "1                           0.389806                      0.334536   \n",
       "2                           1.670997                      1.412687   \n",
       "3                           0.461005                      0.395640   \n",
       "4                           0.271014                      0.266263   \n",
       "...                              ...                           ...   \n",
       "134634                      0.629797                      0.372086   \n",
       "134635                      1.925797                      1.706816   \n",
       "134636                      1.402013                      1.911292   \n",
       "134637                      0.814468                      0.737003   \n",
       "134638                      0.781764                      0.798790   \n",
       "\n",
       "        TransactionAmt_to_mean_card4  TransactionAmt_to_mean_card5  \\\n",
       "0                           0.257761                      0.357614   \n",
       "1                           0.367973                      0.347265   \n",
       "2                           0.573218                      0.839154   \n",
       "3                           0.435185                      0.410694   \n",
       "4                           0.294589                      0.347342   \n",
       "...                              ...                           ...   \n",
       "134634                      0.409277                      0.557002   \n",
       "134635                      1.877415                      1.771762   \n",
       "134636                      2.114622                      1.306741   \n",
       "134637                      0.810668                      0.765047   \n",
       "134638                      0.883768                      1.042027   \n",
       "\n",
       "        TransactionAmt_to_mean_card6  TransactionAmt_to_std_card1  \\\n",
       "0                           0.360461                     0.184566   \n",
       "1                           0.421206                     0.491192   \n",
       "2                           0.399333                     2.636139   \n",
       "3                           0.498140                     3.931656   \n",
       "4                           0.335245                     0.165656   \n",
       "...                              ...                          ...   \n",
       "134634                      0.468484                     0.506516   \n",
       "134635                      1.315552                     0.712097   \n",
       "134636                      1.473155                     0.899892   \n",
       "134637                      0.927942                     0.133132   \n",
       "134638                      1.005736                     0.662777   \n",
       "\n",
       "        TransactionAmt_to_std_card2  TransactionAmt_to_std_card3  \\\n",
       "0                          0.000000                     0.273001   \n",
       "1                          0.237929                     0.195285   \n",
       "2                          2.273508                     1.261766   \n",
       "3                          0.281387                     0.230955   \n",
       "4                          0.204789                     0.155431   \n",
       "...                             ...                          ...   \n",
       "134634                     0.506516                     0.217205   \n",
       "134635                     1.137362                     0.996354   \n",
       "134636                     0.907036                     1.115718   \n",
       "134637                     0.498286                     0.430226   \n",
       "134638                     0.519278                     0.466294   \n",
       "\n",
       "        TransactionAmt_to_std_card4  TransactionAmt_to_std_card5  \\\n",
       "0                          0.170233                     0.205289   \n",
       "1                          0.214723                     0.208132   \n",
       "2                          0.298868                     0.374466   \n",
       "3                          0.253943                     0.246148   \n",
       "4                          0.153595                     0.198215   \n",
       "...                             ...                          ...   \n",
       "134634                     0.238825                     0.402796   \n",
       "134635                     1.095527                     1.061899   \n",
       "134636                     1.102536                     0.647020   \n",
       "134637                     0.473049                     0.458528   \n",
       "134638                     0.460785                     0.594644   \n",
       "\n",
       "        TransactionAmt_to_std_card6   V126         V127        V128       V13  \\\n",
       "0                          0.202121    0.0   117.000000    0.000000  1.000000   \n",
       "1                          0.257389    0.0     0.000000    0.000000  1.000000   \n",
       "2                          0.223918    0.0     0.000000    0.000000  0.000000   \n",
       "3                          0.304402    0.0     0.000000    0.000000  0.000000   \n",
       "4                          0.204861    0.0     0.000000    0.000000  1.000000   \n",
       "...                             ...    ...          ...         ...       ...   \n",
       "134634                     0.286280    0.0     0.000000    0.000000  1.000000   \n",
       "134635                     0.737669    0.0     0.000000    0.000000  0.599166   \n",
       "134636                     0.826042    0.0     0.000000    0.000000  2.000000   \n",
       "134637                     0.567045    0.0   209.949997  209.949997  1.000000   \n",
       "134638                     0.614583  117.0  1035.500000  117.000000  1.000000   \n",
       "\n",
       "              V130        V131       V20      V282      V283  V285  V291  \\\n",
       "0         0.000000    0.000000  1.000000  1.000000  1.000000   0.0   1.0   \n",
       "1         0.000000    0.000000  1.000000  1.000000  1.000000   0.0   1.0   \n",
       "2         0.000000    0.000000  1.000000  4.000000  4.000000   2.0   4.0   \n",
       "3         0.000000    0.000000  1.000000  1.000000  1.000000   0.0   1.0   \n",
       "4         0.000000    0.000000  1.000000  1.000000  1.000000   0.0   1.0   \n",
       "...            ...         ...       ...       ...       ...   ...   ...   \n",
       "134634    0.000000    0.000000  0.000000  0.000000  0.000000   0.0   1.0   \n",
       "134635    0.000000    0.000000  0.847843  0.817171  0.991114   0.0   1.0   \n",
       "134636    0.000000    0.000000  1.000000  1.000000  1.000000   0.0   1.0   \n",
       "134637  209.949997  209.949997  1.000000  1.000000  2.000000   1.0   1.0   \n",
       "134638  117.000000  117.000000  2.000000  2.000000  7.000000   5.0   2.0   \n",
       "\n",
       "        V294        V306         V307        V308        V310        V312  \\\n",
       "0        1.0    0.000000   117.000000    0.000000    0.000000    0.000000   \n",
       "1        0.0    0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "2        0.0  166.215393   166.215393  166.215393   90.327904   90.327904   \n",
       "3        0.0    0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "4        0.0    0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "...      ...         ...          ...         ...         ...         ...   \n",
       "134634   0.0    0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "134635   0.0    0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "134636   0.0    0.000000     0.000000    0.000000    0.000000    0.000000   \n",
       "134637   0.0    0.000000   209.949997  209.949997  209.949997  209.949997   \n",
       "134638  11.0  117.000000  2903.500000  117.000000  669.500000  117.000000   \n",
       "\n",
       "              V313        V314        V315    V317       V35     V38  \\\n",
       "0         0.000000    0.000000    0.000000   117.0  0.542594  1.1624   \n",
       "1         0.000000    0.000000    0.000000     0.0  1.000000  1.0000   \n",
       "2        90.327904   90.327904   90.327904     0.0  0.000000  4.0000   \n",
       "3         0.000000    0.000000    0.000000     0.0  0.542594  1.1624   \n",
       "4         0.000000    0.000000    0.000000     0.0  1.000000  1.0000   \n",
       "...            ...         ...         ...     ...       ...     ...   \n",
       "134634    0.000000    0.000000    0.000000     0.0  1.000000  1.0000   \n",
       "134635   21.351473   43.319174   26.806977     0.0  0.542594  1.1624   \n",
       "134636    0.000000    0.000000    0.000000     0.0  2.000000  1.0000   \n",
       "134637  209.949997  209.949997  209.949997     0.0  1.000000  1.0000   \n",
       "134638  317.500000  669.500000  317.500000  2234.0  1.000000  1.0000   \n",
       "\n",
       "             V44       V45       V53       V54       V56       V61       V62  \\\n",
       "0       1.083891  1.120779  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "2       1.000000  1.000000  0.000000  0.000000  4.000000  1.000000  1.000000   \n",
       "3       1.083891  1.120779  1.000000  1.000000  1.000000  0.000000  0.000000   \n",
       "4       1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "134634  1.000000  1.000000  1.000000  1.000000  1.000000  0.000000  0.000000   \n",
       "134635  1.083891  1.120779  0.577586  0.619982  1.120979  0.829785  0.867563   \n",
       "134636  2.000000  2.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "134637  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "134638  1.000000  1.000000  1.000000  1.000000  1.000000  2.000000  2.000000   \n",
       "\n",
       "             V75       V76       V78      V82       V83       V87  addr1  \\\n",
       "0       1.000000  1.000000  1.000000  0.00000  0.000000  1.000000  315.0   \n",
       "1       1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  272.0   \n",
       "2       0.000000  0.000000  3.000000  1.00000  1.000000  1.000000    0.0   \n",
       "3       1.000000  1.000000  1.000000  0.00000  0.000000  1.000000  315.0   \n",
       "4       1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  299.0   \n",
       "...          ...       ...       ...      ...       ...       ...    ...   \n",
       "134634  1.000000  1.000000  1.000000  0.00000  0.000000  1.000000  325.0   \n",
       "134635  0.544278  0.587557  1.144462  0.84461  0.881965  1.099456  272.0   \n",
       "134636  2.000000  2.000000  1.000000  1.00000  1.000000  2.000000  299.0   \n",
       "134637  1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  485.0   \n",
       "134638  1.000000  1.000000  1.000000  2.00000  2.000000  1.000000  387.0   \n",
       "\n",
       "        card1  card2  card3  card5  dist1  id_01     id_02  id_05  id_06  \\\n",
       "0       13926    0.0  150.0  142.0   19.0    0.0       0.0    0.0    0.0   \n",
       "1        5937  555.0  150.0  226.0   36.0    0.0       0.0    0.0    0.0   \n",
       "2       16496  352.0  117.0  134.0    0.0   -5.0  191631.0    0.0    0.0   \n",
       "3        7055  555.0  150.0  226.0    3.0    0.0       0.0    0.0    0.0   \n",
       "4        7875  314.0  150.0  224.0    0.0    0.0       0.0    0.0    0.0   \n",
       "...       ...    ...    ...    ...    ...    ...       ...    ...    ...   \n",
       "134634   3166  559.0  150.0  166.0    2.0    0.0       0.0    0.0    0.0   \n",
       "134635   1214  174.0  150.0  226.0    0.0   -5.0  172059.0    1.0   -5.0   \n",
       "134636  15066  170.0  150.0  102.0    0.0    0.0       0.0    0.0    0.0   \n",
       "134637  13071  321.0  150.0  226.0  152.0    0.0       0.0    0.0    0.0   \n",
       "134638   7826  481.0  150.0  224.0    3.0    0.0       0.0    0.0    0.0   \n",
       "\n",
       "        id_13  id_19  id_20  id_30  id_31  id_33  M4  M5  M6  P_emaildomain  \\\n",
       "0          55    568    547     86    136    461   2   0   1             32   \n",
       "1          55    568    547     86    136    461   1   0   1             16   \n",
       "2          42    307     42     86     46    461   0   2   2             16   \n",
       "3          55    568    547     86    136    461   3   2   0             32   \n",
       "4          55    568    547     86    136    461   0   0   0             16   \n",
       "...       ...    ...    ...    ...    ...    ...  ..  ..  ..            ...   \n",
       "134634     55    568    547     86    136    461   0   1   1             16   \n",
       "134635     17    249    226      8     33    449   3   2   2             16   \n",
       "134636     55    568    547     86    136    461   3   2   1             16   \n",
       "134637     55    568    547     86    136    461   3   2   0             16   \n",
       "134638     55    568    547     86    136    461   0   0   1              2   \n",
       "\n",
       "        P_emaildomain_2  ProductCD  R_emaildomain  card4  card6  isFraud  \n",
       "0                     7          4             32      1      1        0  \n",
       "1                     2          4             32      4      2        0  \n",
       "2                     2          0             16      2      1        0  \n",
       "3                     7          4             32      4      2        0  \n",
       "4                     2          4             32      2      2        0  \n",
       "...                 ...        ...            ...    ...    ...      ...  \n",
       "134634                2          4             32      4      2        0  \n",
       "134635                2          2             16      4      1        1  \n",
       "134636                2          4             32      2      1        0  \n",
       "134637                2          4             32      4      2        0  \n",
       "134638                2          4             32      2      2        0  \n",
       "\n",
       "[134639 rows x 101 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_downsample_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_train_downsample_top_features = pd.read_csv(\n",
    "    folder_path + \"data_train_downsample_1_to_20_top_features.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C2</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>D1</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>DeviceInfo</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>TransactionAmt_to_mean_card1</th>\n",
       "      <th>TransactionAmt_to_mean_card2</th>\n",
       "      <th>TransactionAmt_to_mean_card3</th>\n",
       "      <th>TransactionAmt_to_mean_card4</th>\n",
       "      <th>TransactionAmt_to_mean_card5</th>\n",
       "      <th>TransactionAmt_to_mean_card6</th>\n",
       "      <th>TransactionAmt_to_std_card1</th>\n",
       "      <th>TransactionAmt_to_std_card2</th>\n",
       "      <th>TransactionAmt_to_std_card3</th>\n",
       "      <th>TransactionAmt_to_std_card4</th>\n",
       "      <th>TransactionAmt_to_std_card5</th>\n",
       "      <th>TransactionAmt_to_std_card6</th>\n",
       "      <th>V126</th>\n",
       "      <th>V127</th>\n",
       "      <th>V128</th>\n",
       "      <th>V13</th>\n",
       "      <th>V130</th>\n",
       "      <th>V131</th>\n",
       "      <th>V20</th>\n",
       "      <th>V282</th>\n",
       "      <th>V283</th>\n",
       "      <th>V285</th>\n",
       "      <th>V291</th>\n",
       "      <th>V294</th>\n",
       "      <th>V306</th>\n",
       "      <th>V307</th>\n",
       "      <th>V308</th>\n",
       "      <th>V310</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V317</th>\n",
       "      <th>V35</th>\n",
       "      <th>V38</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V53</th>\n",
       "      <th>V54</th>\n",
       "      <th>V56</th>\n",
       "      <th>V61</th>\n",
       "      <th>V62</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V78</th>\n",
       "      <th>V82</th>\n",
       "      <th>V83</th>\n",
       "      <th>V87</th>\n",
       "      <th>addr1</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>dist1</th>\n",
       "      <th>id_01</th>\n",
       "      <th>id_02</th>\n",
       "      <th>id_05</th>\n",
       "      <th>id_06</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_30</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_33</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>P_emaildomain_2</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>card4</th>\n",
       "      <th>card6</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>68.500</td>\n",
       "      <td>0.194640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467667</td>\n",
       "      <td>0.257761</td>\n",
       "      <td>0.357614</td>\n",
       "      <td>0.360461</td>\n",
       "      <td>0.184566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273001</td>\n",
       "      <td>0.170233</td>\n",
       "      <td>0.205289</td>\n",
       "      <td>0.202121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.542594</td>\n",
       "      <td>1.1624</td>\n",
       "      <td>1.083891</td>\n",
       "      <td>1.120779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>315.0</td>\n",
       "      <td>13926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>39.000</td>\n",
       "      <td>0.234679</td>\n",
       "      <td>0.271014</td>\n",
       "      <td>0.266263</td>\n",
       "      <td>0.294589</td>\n",
       "      <td>0.347342</td>\n",
       "      <td>0.335245</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.204789</td>\n",
       "      <td>0.155431</td>\n",
       "      <td>0.153595</td>\n",
       "      <td>0.198215</td>\n",
       "      <td>0.204861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>299.0</td>\n",
       "      <td>7875</td>\n",
       "      <td>314.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2526</td>\n",
       "      <td>75.887</td>\n",
       "      <td>1.737993</td>\n",
       "      <td>2.039135</td>\n",
       "      <td>1.412687</td>\n",
       "      <td>0.569886</td>\n",
       "      <td>0.537815</td>\n",
       "      <td>0.399333</td>\n",
       "      <td>2.230553</td>\n",
       "      <td>2.223815</td>\n",
       "      <td>1.261766</td>\n",
       "      <td>0.332545</td>\n",
       "      <td>0.322337</td>\n",
       "      <td>0.223918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13329</td>\n",
       "      <td>569.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>116098.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "      <td>307</td>\n",
       "      <td>42</td>\n",
       "      <td>86</td>\n",
       "      <td>46</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>2454.000</td>\n",
       "      <td>6.018449</td>\n",
       "      <td>16.887618</td>\n",
       "      <td>16.754101</td>\n",
       "      <td>18.536461</td>\n",
       "      <td>21.855849</td>\n",
       "      <td>21.094660</td>\n",
       "      <td>2.964401</td>\n",
       "      <td>8.641732</td>\n",
       "      <td>9.780213</td>\n",
       "      <td>9.664663</td>\n",
       "      <td>12.472285</td>\n",
       "      <td>12.890484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>184.0</td>\n",
       "      <td>2213</td>\n",
       "      <td>556.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>60.000</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.402966</td>\n",
       "      <td>0.409636</td>\n",
       "      <td>0.450580</td>\n",
       "      <td>0.425223</td>\n",
       "      <td>0.515762</td>\n",
       "      <td>0.298385</td>\n",
       "      <td>0.247251</td>\n",
       "      <td>0.239125</td>\n",
       "      <td>0.262927</td>\n",
       "      <td>0.254856</td>\n",
       "      <td>0.315171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>204.0</td>\n",
       "      <td>7207</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49152</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>30.950</td>\n",
       "      <td>0.252113</td>\n",
       "      <td>0.211451</td>\n",
       "      <td>0.211304</td>\n",
       "      <td>0.232424</td>\n",
       "      <td>0.219344</td>\n",
       "      <td>0.266047</td>\n",
       "      <td>0.153613</td>\n",
       "      <td>0.121227</td>\n",
       "      <td>0.123349</td>\n",
       "      <td>0.135626</td>\n",
       "      <td>0.131463</td>\n",
       "      <td>0.162576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>330.0</td>\n",
       "      <td>12932</td>\n",
       "      <td>361.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49153</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1.685142</td>\n",
       "      <td>1.588599</td>\n",
       "      <td>1.542961</td>\n",
       "      <td>1.697183</td>\n",
       "      <td>1.601673</td>\n",
       "      <td>1.942703</td>\n",
       "      <td>0.928316</td>\n",
       "      <td>0.915176</td>\n",
       "      <td>0.900704</td>\n",
       "      <td>0.990357</td>\n",
       "      <td>0.959957</td>\n",
       "      <td>1.187143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>908.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>908.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>204.0</td>\n",
       "      <td>17442</td>\n",
       "      <td>246.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>92.000</td>\n",
       "      <td>1.086637</td>\n",
       "      <td>1.052210</td>\n",
       "      <td>0.628108</td>\n",
       "      <td>0.690889</td>\n",
       "      <td>0.940260</td>\n",
       "      <td>0.790835</td>\n",
       "      <td>1.143114</td>\n",
       "      <td>1.105732</td>\n",
       "      <td>0.366658</td>\n",
       "      <td>0.403154</td>\n",
       "      <td>0.679949</td>\n",
       "      <td>0.483262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.5</td>\n",
       "      <td>108.5</td>\n",
       "      <td>108.5</td>\n",
       "      <td>108.5</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2377</td>\n",
       "      <td>203.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2740</td>\n",
       "      <td>54.500</td>\n",
       "      <td>0.629797</td>\n",
       "      <td>0.629797</td>\n",
       "      <td>0.372086</td>\n",
       "      <td>0.409277</td>\n",
       "      <td>0.557002</td>\n",
       "      <td>0.468484</td>\n",
       "      <td>0.506516</td>\n",
       "      <td>0.506516</td>\n",
       "      <td>0.217205</td>\n",
       "      <td>0.238825</td>\n",
       "      <td>0.402796</td>\n",
       "      <td>0.286280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>325.0</td>\n",
       "      <td>3166</td>\n",
       "      <td>559.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>568</td>\n",
       "      <td>547</td>\n",
       "      <td>86</td>\n",
       "      <td>136</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "      <td>250.000</td>\n",
       "      <td>1.222648</td>\n",
       "      <td>1.925797</td>\n",
       "      <td>1.706816</td>\n",
       "      <td>1.877415</td>\n",
       "      <td>1.771762</td>\n",
       "      <td>1.315552</td>\n",
       "      <td>0.712097</td>\n",
       "      <td>1.137362</td>\n",
       "      <td>0.996354</td>\n",
       "      <td>1.095527</td>\n",
       "      <td>1.061899</td>\n",
       "      <td>0.737669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847843</td>\n",
       "      <td>0.817171</td>\n",
       "      <td>0.991114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.351473</td>\n",
       "      <td>43.319174</td>\n",
       "      <td>26.806977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542594</td>\n",
       "      <td>1.1624</td>\n",
       "      <td>1.083891</td>\n",
       "      <td>1.120779</td>\n",
       "      <td>0.577586</td>\n",
       "      <td>0.619982</td>\n",
       "      <td>1.120979</td>\n",
       "      <td>0.829785</td>\n",
       "      <td>0.867563</td>\n",
       "      <td>0.544278</td>\n",
       "      <td>0.587557</td>\n",
       "      <td>1.144462</td>\n",
       "      <td>0.84461</td>\n",
       "      <td>0.881965</td>\n",
       "      <td>1.099456</td>\n",
       "      <td>272.0</td>\n",
       "      <td>1214</td>\n",
       "      <td>174.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>172059.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>17</td>\n",
       "      <td>249</td>\n",
       "      <td>226</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>449</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49157 rows  101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        C1  C10  C11  C12  C13  C14   C2   C5   C6   C8   C9     D1    D10  \\\n",
       "0      1.0  0.0  2.0  0.0  1.0  1.0  1.0  0.0  1.0  0.0  1.0   14.0   13.0   \n",
       "1      1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  0.0  1.0    0.0    0.0   \n",
       "2      2.0  1.0  2.0  2.0  3.0  2.0  5.0  0.0  1.0  1.0  0.0    0.0    0.0   \n",
       "3      1.0  0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0    0.0    0.0   \n",
       "4      2.0  0.0  1.0  0.0  9.0  2.0  1.0  1.0  1.0  0.0  1.0    0.0  456.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "49152  1.0  0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0    0.0    0.0   \n",
       "49153  3.0  0.0  1.0  0.0  7.0  3.0  2.0  0.0  1.0  0.0  1.0   18.0   19.0   \n",
       "49154  1.0  0.0  1.0  0.0  1.0  1.0  2.0  0.0  2.0  0.0  1.0    3.0    0.0   \n",
       "49155  2.0  0.0  1.0  0.0  7.0  2.0  1.0  0.0  1.0  0.0  2.0  309.0  309.0   \n",
       "49156  1.0  1.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0    0.0    0.0   \n",
       "\n",
       "         D11  D12  D13  D14    D15     D2    D3     D4    D5   D6    D8   D9  \\\n",
       "0       13.0  0.0  0.0  0.0    0.0    0.0  13.0    0.0   0.0  0.0   0.0  0.0   \n",
       "1        0.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0   \n",
       "2        0.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0  83.0  0.0   \n",
       "3        0.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0   \n",
       "4        0.0  0.0  0.0  0.0  456.0    0.0   0.0  456.0  32.0  0.0   0.0  0.0   \n",
       "...      ...  ...  ...  ...    ...    ...   ...    ...   ...  ...   ...  ...   \n",
       "49152    0.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0   \n",
       "49153  226.0  0.0  0.0  0.0  226.0   18.0   3.0  226.0  18.0  0.0   0.0  0.0   \n",
       "49154    4.0  0.0  0.0  0.0    4.0    3.0   4.0    0.0   0.0  0.0   0.0  0.0   \n",
       "49155  448.0  0.0  0.0  0.0  309.0  309.0  32.0  448.0  32.0  0.0   0.0  0.0   \n",
       "49156    0.0  0.0  0.0  0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0   \n",
       "\n",
       "       DeviceInfo  TransactionAmt  TransactionAmt_to_mean_card1  \\\n",
       "0            2740          68.500                      0.194640   \n",
       "1            2740          39.000                      0.234679   \n",
       "2            2526          75.887                      1.737993   \n",
       "3            2740        2454.000                      6.018449   \n",
       "4            2740          60.000                      0.425700   \n",
       "...           ...             ...                           ...   \n",
       "49152        2740          30.950                      0.252113   \n",
       "49153        2740         226.000                      1.685142   \n",
       "49154        2740          92.000                      1.086637   \n",
       "49155        2740          54.500                      0.629797   \n",
       "49156         116         250.000                      1.222648   \n",
       "\n",
       "       TransactionAmt_to_mean_card2  TransactionAmt_to_mean_card3  \\\n",
       "0                          0.000000                      0.467667   \n",
       "1                          0.271014                      0.266263   \n",
       "2                          2.039135                      1.412687   \n",
       "3                         16.887618                     16.754101   \n",
       "4                          0.402966                      0.409636   \n",
       "...                             ...                           ...   \n",
       "49152                      0.211451                      0.211304   \n",
       "49153                      1.588599                      1.542961   \n",
       "49154                      1.052210                      0.628108   \n",
       "49155                      0.629797                      0.372086   \n",
       "49156                      1.925797                      1.706816   \n",
       "\n",
       "       TransactionAmt_to_mean_card4  TransactionAmt_to_mean_card5  \\\n",
       "0                          0.257761                      0.357614   \n",
       "1                          0.294589                      0.347342   \n",
       "2                          0.569886                      0.537815   \n",
       "3                         18.536461                     21.855849   \n",
       "4                          0.450580                      0.425223   \n",
       "...                             ...                           ...   \n",
       "49152                      0.232424                      0.219344   \n",
       "49153                      1.697183                      1.601673   \n",
       "49154                      0.690889                      0.940260   \n",
       "49155                      0.409277                      0.557002   \n",
       "49156                      1.877415                      1.771762   \n",
       "\n",
       "       TransactionAmt_to_mean_card6  TransactionAmt_to_std_card1  \\\n",
       "0                          0.360461                     0.184566   \n",
       "1                          0.335245                     0.165656   \n",
       "2                          0.399333                     2.230553   \n",
       "3                         21.094660                     2.964401   \n",
       "4                          0.515762                     0.298385   \n",
       "...                             ...                          ...   \n",
       "49152                      0.266047                     0.153613   \n",
       "49153                      1.942703                     0.928316   \n",
       "49154                      0.790835                     1.143114   \n",
       "49155                      0.468484                     0.506516   \n",
       "49156                      1.315552                     0.712097   \n",
       "\n",
       "       TransactionAmt_to_std_card2  TransactionAmt_to_std_card3  \\\n",
       "0                         0.000000                     0.273001   \n",
       "1                         0.204789                     0.155431   \n",
       "2                         2.223815                     1.261766   \n",
       "3                         8.641732                     9.780213   \n",
       "4                         0.247251                     0.239125   \n",
       "...                            ...                          ...   \n",
       "49152                     0.121227                     0.123349   \n",
       "49153                     0.915176                     0.900704   \n",
       "49154                     1.105732                     0.366658   \n",
       "49155                     0.506516                     0.217205   \n",
       "49156                     1.137362                     0.996354   \n",
       "\n",
       "       TransactionAmt_to_std_card4  TransactionAmt_to_std_card5  \\\n",
       "0                         0.170233                     0.205289   \n",
       "1                         0.153595                     0.198215   \n",
       "2                         0.332545                     0.322337   \n",
       "3                         9.664663                    12.472285   \n",
       "4                         0.262927                     0.254856   \n",
       "...                            ...                          ...   \n",
       "49152                     0.135626                     0.131463   \n",
       "49153                     0.990357                     0.959957   \n",
       "49154                     0.403154                     0.679949   \n",
       "49155                     0.238825                     0.402796   \n",
       "49156                     1.095527                     1.061899   \n",
       "\n",
       "       TransactionAmt_to_std_card6  V126    V127   V128       V13   V130  \\\n",
       "0                         0.202121   0.0   117.0    0.0  1.000000    0.0   \n",
       "1                         0.204861   0.0     0.0    0.0  1.000000    0.0   \n",
       "2                         0.223918   0.0     0.0    0.0  0.000000    0.0   \n",
       "3                        12.890484   0.0     0.0    0.0  0.000000    0.0   \n",
       "4                         0.315171   0.0     0.0    0.0  0.000000    0.0   \n",
       "...                            ...   ...     ...    ...       ...    ...   \n",
       "49152                     0.162576   0.0     0.0    0.0  1.000000    0.0   \n",
       "49153                     1.187143   0.0  1112.0  816.0  1.000000  908.0   \n",
       "49154                     0.483262   0.0     0.0    0.0  1.000000    0.0   \n",
       "49155                     0.286280   0.0     0.0    0.0  1.000000    0.0   \n",
       "49156                     0.737669   0.0     0.0    0.0  0.599166    0.0   \n",
       "\n",
       "        V131       V20      V282      V283  V285  V291  V294  V306    V307  \\\n",
       "0        0.0  1.000000  1.000000  1.000000   0.0   1.0   1.0   0.0   117.0   \n",
       "1        0.0  1.000000  1.000000  1.000000   0.0   1.0   0.0   0.0     0.0   \n",
       "2        0.0  1.000000  1.000000  1.000000   0.0   1.0   0.0   0.0     0.0   \n",
       "3        0.0  1.000000  1.000000  1.000000   0.0   1.0   0.0   0.0     0.0   \n",
       "4        0.0  1.000000  1.000000  1.000000   0.0   1.0   0.0   0.0     0.0   \n",
       "...      ...       ...       ...       ...   ...   ...   ...   ...     ...   \n",
       "49152    0.0  1.000000  1.000000  1.000000   0.0   1.0   0.0   0.0     0.0   \n",
       "49153  816.0  1.000000  3.000000  3.000000   4.0   1.0   1.0   0.0  1112.0   \n",
       "49154    0.0  1.000000  1.000000  2.000000   1.0   2.0   0.0   0.0   108.5   \n",
       "49155    0.0  0.000000  0.000000  0.000000   0.0   1.0   0.0   0.0     0.0   \n",
       "49156    0.0  0.847843  0.817171  0.991114   0.0   1.0   0.0   0.0     0.0   \n",
       "\n",
       "        V308   V310   V312        V313        V314        V315   V317  \\\n",
       "0        0.0    0.0    0.0    0.000000    0.000000    0.000000  117.0   \n",
       "1        0.0    0.0    0.0    0.000000    0.000000    0.000000    0.0   \n",
       "2        0.0    0.0    0.0    0.000000    0.000000    0.000000    0.0   \n",
       "3        0.0    0.0    0.0    0.000000    0.000000    0.000000    0.0   \n",
       "4        0.0    0.0    0.0    0.000000    0.000000    0.000000    0.0   \n",
       "...      ...    ...    ...         ...         ...         ...    ...   \n",
       "49152    0.0    0.0    0.0    0.000000    0.000000    0.000000    0.0   \n",
       "49153  816.0  908.0  816.0   92.000000  908.000000   92.000000  102.0   \n",
       "49154  108.5  108.5  108.5  108.500000  108.500000  108.500000    0.0   \n",
       "49155    0.0    0.0    0.0    0.000000    0.000000    0.000000    0.0   \n",
       "49156    0.0    0.0    0.0   21.351473   43.319174   26.806977    0.0   \n",
       "\n",
       "            V35     V38       V44       V45       V53       V54       V56  \\\n",
       "0      0.542594  1.1624  1.083891  1.120779  1.000000  1.000000  1.000000   \n",
       "1      1.000000  1.0000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "2      0.000000  5.0000  2.000000  2.000000  0.000000  0.000000  1.000000   \n",
       "3      0.000000  1.0000  1.000000  1.000000  0.000000  0.000000  1.000000   \n",
       "4      0.000000  1.0000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "...         ...     ...       ...       ...       ...       ...       ...   \n",
       "49152  1.000000  1.0000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "49153  1.000000  1.0000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "49154  1.000000  1.0000  1.000000  1.000000  1.000000  1.000000  2.000000   \n",
       "49155  1.000000  1.0000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "49156  0.542594  1.1624  1.083891  1.120779  0.577586  0.619982  1.120979   \n",
       "\n",
       "            V61       V62       V75       V76       V78      V82       V83  \\\n",
       "0      1.000000  1.000000  1.000000  1.000000  1.000000  0.00000  0.000000   \n",
       "1      1.000000  1.000000  1.000000  1.000000  1.000000  1.00000  1.000000   \n",
       "2      1.000000  1.000000  0.000000  0.000000  4.000000  1.00000  1.000000   \n",
       "3      1.000000  1.000000  0.000000  0.000000  1.000000  1.00000  1.000000   \n",
       "4      1.000000  1.000000  0.000000  0.000000  1.000000  1.00000  1.000000   \n",
       "...         ...       ...       ...       ...       ...      ...       ...   \n",
       "49152  1.000000  1.000000  1.000000  1.000000  1.000000  1.00000  1.000000   \n",
       "49153  1.000000  1.000000  1.000000  1.000000  1.000000  1.00000  1.000000   \n",
       "49154  1.000000  2.000000  1.000000  1.000000  2.000000  1.00000  2.000000   \n",
       "49155  0.000000  0.000000  1.000000  1.000000  1.000000  0.00000  0.000000   \n",
       "49156  0.829785  0.867563  0.544278  0.587557  1.144462  0.84461  0.881965   \n",
       "\n",
       "            V87  addr1  card1  card2  card3  card5   dist1  id_01     id_02  \\\n",
       "0      1.000000  315.0  13926    0.0  150.0  142.0    19.0    0.0       0.0   \n",
       "1      1.000000  299.0   7875  314.0  150.0  224.0     0.0    0.0       0.0   \n",
       "2      2.000000    0.0  13329  569.0  117.0  226.0     0.0  -10.0  116098.0   \n",
       "3      1.000000  184.0   2213  556.0  150.0  224.0     0.0    0.0       0.0   \n",
       "4      1.000000  204.0   7207  111.0  150.0  226.0     0.0    0.0       0.0   \n",
       "...         ...    ...    ...    ...    ...    ...     ...    ...       ...   \n",
       "49152  1.000000  330.0  12932  361.0  150.0  226.0     0.0    0.0       0.0   \n",
       "49153  1.000000  204.0  17442  246.0  150.0  226.0  1022.0    0.0       0.0   \n",
       "49154  1.000000  110.0   2377  203.0  150.0  166.0     0.0    0.0       0.0   \n",
       "49155  1.000000  325.0   3166  559.0  150.0  166.0     2.0    0.0       0.0   \n",
       "49156  1.099456  272.0   1214  174.0  150.0  226.0     0.0   -5.0  172059.0   \n",
       "\n",
       "       id_05  id_06  id_13  id_19  id_20  id_30  id_31  id_33  M4  M5  M6  \\\n",
       "0        0.0    0.0     55    568    547     86    136    461   2   0   1   \n",
       "1        0.0    0.0     55    568    547     86    136    461   0   0   0   \n",
       "2        0.0    0.0     42    307     42     86     46    461   0   2   2   \n",
       "3        0.0    0.0     55    568    547     86    136    461   3   2   1   \n",
       "4        0.0    0.0     55    568    547     86    136    461   3   2   0   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  ..  ..  ..   \n",
       "49152    0.0    0.0     55    568    547     86    136    461   3   2   1   \n",
       "49153    0.0    0.0     55    568    547     86    136    461   1   0   0   \n",
       "49154    0.0    0.0     55    568    547     86    136    461   2   0   1   \n",
       "49155    0.0    0.0     55    568    547     86    136    461   0   1   1   \n",
       "49156    1.0   -5.0     17    249    226      8     33    449   3   2   2   \n",
       "\n",
       "       P_emaildomain  P_emaildomain_2  ProductCD  R_emaildomain  card4  card6  \\\n",
       "0                 32                7          4             32      1      1   \n",
       "1                 16                2          4             32      2      2   \n",
       "2                 16                2          0             16      4      1   \n",
       "3                 19                2          4             32      2      2   \n",
       "4                  9                8          4             32      4      2   \n",
       "...              ...              ...        ...            ...    ...    ...   \n",
       "49152             16                2          4             32      4      2   \n",
       "49153             19                2          4             32      4      2   \n",
       "49154             23                2          4             32      4      2   \n",
       "49155             16                2          4             32      4      2   \n",
       "49156             16                2          2             16      4      1   \n",
       "\n",
       "       isFraud  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "49152        0  \n",
       "49153        0  \n",
       "49154        0  \n",
       "49155        0  \n",
       "49156        1  \n",
       "\n",
       "[49157 rows x 101 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_downsample_top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# try of train on all data: too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_top_features),\n",
    "        (\"scaling\", StandardScaler(), numeric_top_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_transf = column_transformer.fit_transform(X)\n",
    "X_test_transf = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 246)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 246)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_transf, target_train, test_size=0.2, random_state=0, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV 4/5; 2/18] START n_neighbors=1, p=2.........................................\n",
      "[CV 1/5; 2/18] START n_neighbors=1, p=2.........................................\n",
      "[CV 1/5; 1/18] START n_neighbors=1, p=1.........................................\n",
      "[CV 3/5; 1/18] START n_neighbors=1, p=1.........................................\n",
      "[CV 2/5; 2/18] START n_neighbors=1, p=2.........................................\n",
      "[CV 1/5; 3/18] START n_neighbors=1, p=3.........................................\n",
      "[CV 5/5; 1/18] START n_neighbors=1, p=1.........................................\n",
      "[CV 4/5; 1/18] START n_neighbors=1, p=1.........................................\n",
      "[CV 3/5; 2/18] START n_neighbors=1, p=2.........................................\n",
      "[CV 5/5; 2/18] START n_neighbors=1, p=2.........................................\n",
      "[CV 2/5; 1/18] START n_neighbors=1, p=1.........................................\n",
      "[CV 2/5; 3/18] START n_neighbors=1, p=3.........................................\n",
      "[CV 1/5; 2/18] END ..........n_neighbors=1, p=2;, score=0.635 total time=73.8min\n",
      "[CV 3/5; 3/18] START n_neighbors=1, p=3.........................................\n",
      "[CV 5/5; 2/18] END ..........n_neighbors=1, p=2;, score=0.658 total time=73.9min\n",
      "[CV 4/5; 3/18] START n_neighbors=1, p=3.........................................\n",
      "[CV 2/5; 2/18] END ..........n_neighbors=1, p=2;, score=0.686 total time=73.9min\n",
      "[CV 5/5; 3/18] START n_neighbors=1, p=3.........................................\n",
      "[CV 4/5; 2/18] END ..........n_neighbors=1, p=2;, score=0.665 total time=74.0min\n",
      "[CV 1/5; 4/18] START n_neighbors=3, p=1.........................................\n",
      "[CV 3/5; 2/18] END ..........n_neighbors=1, p=2;, score=0.674 total time=74.0min\n",
      "[CV 2/5; 4/18] START n_neighbors=3, p=1.........................................\n",
      "[CV 2/5; 1/18] END .........n_neighbors=1, p=1;, score=0.703 total time=179.3min\n",
      "[CV 3/5; 4/18] START n_neighbors=3, p=1.........................................\n",
      "[CV 5/5; 1/18] END .........n_neighbors=1, p=1;, score=0.673 total time=179.3min\n",
      "[CV 4/5; 4/18] START n_neighbors=3, p=1.........................................\n",
      "[CV 3/5; 1/18] END .........n_neighbors=1, p=1;, score=0.706 total time=179.3min\n",
      "[CV 5/5; 4/18] START n_neighbors=3, p=1.........................................\n",
      "[CV 4/5; 1/18] END .........n_neighbors=1, p=1;, score=0.682 total time=179.4min\n",
      "[CV 1/5; 5/18] START n_neighbors=3, p=2.........................................\n",
      "[CV 1/5; 1/18] END .........n_neighbors=1, p=1;, score=0.647 total time=179.5min\n",
      "[CV 2/5; 5/18] START n_neighbors=3, p=2.........................................\n",
      "[CV 1/5; 5/18] END ..........n_neighbors=3, p=2;, score=0.692 total time=14.2min\n",
      "[CV 3/5; 5/18] START n_neighbors=3, p=2.........................................\n",
      "[CV 2/5; 5/18] END ..........n_neighbors=3, p=2;, score=0.735 total time=14.2min\n",
      "[CV 4/5; 5/18] START n_neighbors=3, p=2.........................................\n",
      "[CV 2/5; 4/18] END .........n_neighbors=3, p=1;, score=0.758 total time=123.3min\n",
      "[CV 5/5; 5/18] START n_neighbors=3, p=2.........................................\n",
      "[CV 1/5; 4/18] END .........n_neighbors=3, p=1;, score=0.703 total time=123.5min\n",
      "[CV 1/5; 6/18] START n_neighbors=3, p=3.........................................\n",
      "[CV 3/5; 5/18] END ..........n_neighbors=3, p=2;, score=0.719 total time=14.4min\n",
      "[CV 2/5; 6/18] START n_neighbors=3, p=3.........................................\n",
      "[CV 4/5; 5/18] END ..........n_neighbors=3, p=2;, score=0.715 total time=14.4min\n",
      "[CV 3/5; 6/18] START n_neighbors=3, p=3.........................................\n",
      "[CV 5/5; 5/18] END ..........n_neighbors=3, p=2;, score=0.710 total time=14.4min\n",
      "[CV 4/5; 6/18] START n_neighbors=3, p=3.........................................\n",
      "[CV 3/5; 4/18] END .........n_neighbors=3, p=1;, score=0.762 total time=121.8min\n",
      "[CV 5/5; 6/18] START n_neighbors=3, p=3.........................................\n",
      "[CV 4/5; 4/18] END .........n_neighbors=3, p=1;, score=0.735 total time=121.9min\n",
      "[CV 1/5; 7/18] START n_neighbors=5, p=1.........................................\n",
      "[CV 5/5; 4/18] END .........n_neighbors=3, p=1;, score=0.726 total time=122.0min\n",
      "[CV 2/5; 7/18] START n_neighbors=5, p=1.........................................\n",
      "[CV 1/5; 7/18] END .........n_neighbors=5, p=1;, score=0.731 total time=120.5min\n",
      "[CV 3/5; 7/18] START n_neighbors=5, p=1.........................................\n",
      "[CV 2/5; 7/18] END .........n_neighbors=5, p=1;, score=0.782 total time=120.6min\n",
      "[CV 4/5; 7/18] START n_neighbors=5, p=1.........................................\n",
      "[CV 3/5; 7/18] END .........n_neighbors=5, p=1;, score=0.791 total time=120.4min\n",
      "[CV 5/5; 7/18] START n_neighbors=5, p=1.........................................\n",
      "[CV 4/5; 7/18] END .........n_neighbors=5, p=1;, score=0.762 total time=120.4min\n",
      "[CV 1/5; 8/18] START n_neighbors=5, p=2.........................................\n",
      "[CV 1/5; 8/18] END ..........n_neighbors=5, p=2;, score=0.713 total time=13.9min\n",
      "[CV 2/5; 8/18] START n_neighbors=5, p=2.........................................\n",
      "[CV 2/5; 8/18] END ..........n_neighbors=5, p=2;, score=0.762 total time=13.9min\n",
      "[CV 3/5; 8/18] START n_neighbors=5, p=2.........................................\n",
      "[CV 3/5; 8/18] END ..........n_neighbors=5, p=2;, score=0.742 total time=13.9min\n",
      "[CV 4/5; 8/18] START n_neighbors=5, p=2.........................................\n",
      "[CV 4/5; 8/18] END ..........n_neighbors=5, p=2;, score=0.738 total time=13.8min\n",
      "[CV 5/5; 8/18] START n_neighbors=5, p=2.........................................\n",
      "[CV 5/5; 8/18] END ..........n_neighbors=5, p=2;, score=0.735 total time=13.9min\n",
      "[CV 1/5; 9/18] START n_neighbors=5, p=3.........................................\n",
      "[CV 5/5; 7/18] END .........n_neighbors=5, p=1;, score=0.753 total time=120.3min\n",
      "[CV 2/5; 9/18] START n_neighbors=5, p=3.........................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m search_space \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m15\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m: [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)]}\n\u001b[1;32m      2\u001b[0m grid_search_knn \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m      3\u001b[0m     KNeighborsClassifier(), search_space, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m \u001b[43mgrid_search_knn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# search_space = {\"n_neighbors\": [1, 3, 5, 8, 11, 15], \"p\": [i for i in range(1, 4)]}\n",
    "# grid_search_knn = GridSearchCV(\n",
    "#     KNeighborsClassifier(),\n",
    "#     search_space,\n",
    "#     scoring=\"roc_auc\",\n",
    "#     refit=True,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=10,\n",
    "# )\n",
    "# grid_search_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train on downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_downsample = data_train_downsample_top_features.drop(columns=[\"isFraud\"])\n",
    "y_downsample = data_train_downsample_top_features[\"isFraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_top_features),\n",
    "        (\"scaling\", StandardScaler(), numeric_top_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_transf_downsample = column_transformer.fit_transform(X_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(\n",
    "    X_train_downsample,\n",
    "    X_val_downsample,\n",
    "    y_train_downsample,\n",
    "    y_val_downsample,\n",
    ") = train_test_split(\n",
    "    X_train_transf_downsample,\n",
    "    y_downsample,\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     \"n_neighbors\": [\n",
    "#         5,\n",
    "#     ],\n",
    "#     \"p\": [i for i in range(1, 3)],\n",
    "# }\n",
    "# grid_search_knn = GridSearchCV(\n",
    "#     KNeighborsClassifier(), search_space, scoring=\"roc_auc\", n_jobs=-1, verbose=10, cv=4\n",
    "# )\n",
    "# # there so long to calc with this num of examples\n",
    "# grid_search_knn.fit(X_train_downsample[::5], y_train_downsample[::5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"Best parameters: {grid_search_knn.best_params_}\")\n",
    "# print(f\"Best roc_auc score: {grid_search_knn.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:31:41,045] A new study created in memory with name: no-name-943a3d82-a961-4ca2-bf45-324d08de9e3a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d50b1eecdeb4f9e9cb5622966cb0b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:31:42,620] Trial 0 finished with value: 0.7881765106198142 and parameters: {'n_neighbors': 4, 'p': 2}. Best is trial 0 with value: 0.7881765106198142.\n",
      "[I 2024-12-16 19:32:16,195] Trial 1 finished with value: 0.7705856568881357 and parameters: {'n_neighbors': 8, 'p': 4}. Best is trial 0 with value: 0.7881765106198142.\n",
      "[I 2024-12-16 19:32:49,015] Trial 2 finished with value: 0.7722703236385517 and parameters: {'n_neighbors': 9, 'p': 4}. Best is trial 0 with value: 0.7881765106198142.\n",
      "[I 2024-12-16 19:32:49,303] Trial 3 finished with value: 0.6975072598337932 and parameters: {'n_neighbors': 1, 'p': 2}. Best is trial 0 with value: 0.7881765106198142.\n",
      "[I 2024-12-16 19:32:49,692] Trial 4 finished with value: 0.7925848520636911 and parameters: {'n_neighbors': 5, 'p': 2}. Best is trial 4 with value: 0.7925848520636911.\n",
      "[I 2024-12-16 19:32:53,663] Trial 5 finished with value: 0.8195505206552343 and parameters: {'n_neighbors': 7, 'p': 1}. Best is trial 5 with value: 0.8195505206552343.\n",
      "[I 2024-12-16 19:32:54,073] Trial 6 finished with value: 0.6975072598337932 and parameters: {'n_neighbors': 1, 'p': 2}. Best is trial 5 with value: 0.8195505206552343.\n",
      "[I 2024-12-16 19:33:26,781] Trial 7 finished with value: 0.7689152690937571 and parameters: {'n_neighbors': 4, 'p': 3}. Best is trial 5 with value: 0.8195505206552343.\n",
      "[I 2024-12-16 19:33:27,079] Trial 8 finished with value: 0.7770934269397226 and parameters: {'n_neighbors': 3, 'p': 2}. Best is trial 5 with value: 0.8195505206552343.\n",
      "[I 2024-12-16 19:33:59,848] Trial 9 finished with value: 0.7843105537886428 and parameters: {'n_neighbors': 8, 'p': 3}. Best is trial 5 with value: 0.8195505206552343.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 10)\n",
    "    p = trial.suggest_int(\"p\", 1, 4)\n",
    "\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors,\n",
    "        p=p,\n",
    "    )\n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train_downsample[::5],\n",
    "        y_train_downsample[::5],\n",
    "        cv=4,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    roc_auc = score.mean()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : {'n_neighbors': 7, 'p': 1}\n",
      "  roc auc: 0.8195505206552343\n"
     ]
    }
   ],
   "source": [
    "print(f\" : {study.best_params}\")\n",
    "# {'n_neighbors': 7, 'p': 1}\n",
    "print(f\"  roc auc: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all data with this params and check at val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"loss\": [\"hinge\", \"squared_hinge\"],\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid_search_linear = GridSearchCV(\n",
    "#     LinearSVC(random_state=42, penalty=\"l2\", max_iter=100000),\n",
    "#     search_space,\n",
    "#     scoring=\"roc_auc\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1,\n",
    "#     cv=4,\n",
    "# )\n",
    "# grid_search_linear.fit(X_train_downsample, y_train_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Best parameters: {grid_search_linear.best_params_}\")\n",
    "print(f\"Best roc_auc score: {grid_search_linear.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:34:09,073] A new study created in memory with name: no-name-adbc29e9-0842-4430-9e8c-b1ff5d381931\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e9be0be58e48ac9bbc9c3e8cc1d238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:36:13,798] Trial 0 finished with value: 0.825058602100345 and parameters: {'loss': 'hinge', 'C': 4.408454538699902}. Best is trial 0 with value: 0.825058602100345.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:36:57,557] Trial 1 finished with value: 0.823888894925006 and parameters: {'loss': 'hinge', 'C': 1.2280540920035665}. Best is trial 0 with value: 0.825058602100345.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:39:12,275] Trial 2 finished with value: 0.8250304737031277 and parameters: {'loss': 'hinge', 'C': 4.967679766573312}. Best is trial 0 with value: 0.825058602100345.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:40:49,159] Trial 3 finished with value: 0.8247601937025641 and parameters: {'loss': 'hinge', 'C': 3.282885846888436}. Best is trial 0 with value: 0.825058602100345.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:42:05,517] Trial 4 finished with value: 0.8245088821700213 and parameters: {'loss': 'hinge', 'C': 2.4098211121590047}. Best is trial 0 with value: 0.825058602100345.\n",
      "[I 2024-12-16 19:42:26,097] Trial 5 finished with value: 0.825989985015489 and parameters: {'loss': 'squared_hinge', 'C': 1.2302151839499136}. Best is trial 5 with value: 0.825989985015489.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:43:09,432] Trial 6 finished with value: 0.8237205556381979 and parameters: {'loss': 'hinge', 'C': 1.2200829430875377}. Best is trial 5 with value: 0.825989985015489.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:44:12,179] Trial 7 finished with value: 0.8243133897403329 and parameters: {'loss': 'hinge', 'C': 1.950136484614543}. Best is trial 5 with value: 0.825989985015489.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mlcore/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1243: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:44:52,536] Trial 8 finished with value: 0.8230626886125493 and parameters: {'loss': 'hinge', 'C': 0.6556451845625354}. Best is trial 5 with value: 0.825989985015489.\n",
      "[I 2024-12-16 19:45:09,339] Trial 9 finished with value: 0.826003627347705 and parameters: {'loss': 'squared_hinge', 'C': 3.4668333753961114}. Best is trial 9 with value: 0.826003627347705.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    loss = trial.suggest_categorical(\"loss\", [\"hinge\", \"squared_hinge\"])\n",
    "    C = trial.suggest_float(\"C\", 0.01, 5)\n",
    "\n",
    "    model = LinearSVC(random_state=42, penalty=\"l2\", max_iter=100000, C=C, loss=loss)\n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train_downsample,\n",
    "        y_train_downsample,\n",
    "        cv=4,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    roc_auc = score.mean()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study_linear = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "study_linear.optimize(objective, n_trials=10, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : {'loss': 'squared_hinge', 'C': 3.4668333753961114}\n",
      "  roc auc: 0.826003627347705\n"
     ]
    }
   ],
   "source": [
    "print(f\" : {study_linear.best_params}\")\n",
    "# {'n_neighbors': 7, 'p': 1}\n",
    "print(f\"  roc auc: {study_linear.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"max_depth\": [5, 7, 9, 12],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_tree = GridSearchCV(\n",
    "#     DecisionTreeClassifier(random_state=42),\n",
    "#     search_space,\n",
    "#     scoring=\"roc_auc\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=10,\n",
    "#     cv=4,\n",
    "# )\n",
    "# grid_search_tree.fit(X_train_downsample, y_train_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Best parameters: {grid_search_tree.best_params_}\")\n",
    "# print(f\"Best roc_auc score: {grid_search_tree.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:46:07,722] A new study created in memory with name: no-name-14f6247b-a927-4ecf-9373-8a3cb994a611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7654dfd38054fa38a6f69a129a9273d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:46:09,907] Trial 0 finished with value: 0.8244025776574874 and parameters: {'criterion': 'log_loss', 'max_depth': 10}. Best is trial 0 with value: 0.8244025776574874.\n",
      "[I 2024-12-16 19:46:11,664] Trial 1 finished with value: 0.8311011877359784 and parameters: {'criterion': 'entropy', 'max_depth': 9}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:12,633] Trial 2 finished with value: 0.8279488597953473 and parameters: {'criterion': 'log_loss', 'max_depth': 5}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:14,094] Trial 3 finished with value: 0.8051324039503882 and parameters: {'criterion': 'entropy', 'max_depth': 12}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:15,743] Trial 4 finished with value: 0.8051324039503882 and parameters: {'criterion': 'entropy', 'max_depth': 12}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:17,172] Trial 5 finished with value: 0.8051324039503882 and parameters: {'criterion': 'log_loss', 'max_depth': 12}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:18,653] Trial 6 finished with value: 0.8051324039503882 and parameters: {'criterion': 'entropy', 'max_depth': 12}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:20,133] Trial 7 finished with value: 0.8051324039503882 and parameters: {'criterion': 'log_loss', 'max_depth': 12}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:21,360] Trial 8 finished with value: 0.8311011877359784 and parameters: {'criterion': 'entropy', 'max_depth': 9}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:22,528] Trial 9 finished with value: 0.826577021225285 and parameters: {'criterion': 'gini', 'max_depth': 8}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:23,439] Trial 10 finished with value: 0.8291833758293017 and parameters: {'criterion': 'gini', 'max_depth': 6}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:24,752] Trial 11 finished with value: 0.8311011877359784 and parameters: {'criterion': 'entropy', 'max_depth': 9}. Best is trial 1 with value: 0.8311011877359784.\n",
      "[I 2024-12-16 19:46:25,945] Trial 12 finished with value: 0.8345420454890244 and parameters: {'criterion': 'entropy', 'max_depth': 8}. Best is trial 12 with value: 0.8345420454890244.\n",
      "[I 2024-12-16 19:46:26,997] Trial 13 finished with value: 0.8362867413049774 and parameters: {'criterion': 'entropy', 'max_depth': 7}. Best is trial 13 with value: 0.8362867413049774.\n",
      "[I 2024-12-16 19:46:28,079] Trial 14 finished with value: 0.8362867413049774 and parameters: {'criterion': 'entropy', 'max_depth': 7}. Best is trial 13 with value: 0.8362867413049774.\n",
      "[I 2024-12-16 19:46:29,120] Trial 15 finished with value: 0.8362867413049774 and parameters: {'criterion': 'entropy', 'max_depth': 7}. Best is trial 13 with value: 0.8362867413049774.\n",
      "[I 2024-12-16 19:46:30,020] Trial 16 finished with value: 0.8291833758293017 and parameters: {'criterion': 'gini', 'max_depth': 6}. Best is trial 13 with value: 0.8362867413049774.\n",
      "[I 2024-12-16 19:46:31,072] Trial 17 finished with value: 0.8362867413049774 and parameters: {'criterion': 'entropy', 'max_depth': 7}. Best is trial 13 with value: 0.8362867413049774.\n",
      "[I 2024-12-16 19:46:31,912] Trial 18 finished with value: 0.8279488597953473 and parameters: {'criterion': 'entropy', 'max_depth': 5}. Best is trial 13 with value: 0.8362867413049774.\n",
      "[I 2024-12-16 19:46:32,933] Trial 19 finished with value: 0.8318251297845123 and parameters: {'criterion': 'gini', 'max_depth': 7}. Best is trial 13 with value: 0.8362867413049774.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 12)\n",
    "    model = DecisionTreeClassifier(\n",
    "        random_state=42, max_depth=max_depth, criterion=criterion\n",
    "    )\n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train_downsample,\n",
    "        y_train_downsample,\n",
    "        cv=4,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    roc_auc = score.mean()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study_tree = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "study_tree.optimize(objective, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : {'criterion': 'entropy', 'max_depth': 7}\n",
      "  roc auc: 0.8362867413049774\n"
     ]
    }
   ],
   "source": [
    "print(f\" : {study_tree.best_params}\")\n",
    "# {'n_neighbors': 7, 'p': 1}\n",
    "print(f\"  roc auc: {study_tree.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging\n",
    "  ,      (  - )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"estimator__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"estimator__min_samples_leaf\": [1, 3, 5, 7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_bagging = GridSearchCV(\n",
    "#     BaggingClassifier(\n",
    "#         estimator=DecisionTreeClassifier(random_state=42),\n",
    "#         n_estimators=200,\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1,\n",
    "#     ),\n",
    "#     search_space,\n",
    "#     scoring=\"roc_auc\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=10,\n",
    "#     cv=4,\n",
    "# )\n",
    "# grid_search_bagging.fit(X_train_downsample, y_train_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Best parameters: {grid_search_bagging.best_params_}\")\n",
    "# print(f\"Best roc_auc score: {grid_search_bagging.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:51:41,149] A new study created in memory with name: no-name-b8a45f86-ed47-4968-992b-56cdc510cd99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd37bb61a8f441a9a09a5f142d595a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:52:00,467] Trial 0 finished with value: 0.8889949791128808 and parameters: {'criterion': 'gini', 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8889949791128808.\n",
      "[I 2024-12-16 19:52:17,898] Trial 1 finished with value: 0.890695162674664 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 1}. Best is trial 1 with value: 0.890695162674664.\n",
      "[I 2024-12-16 19:52:34,825] Trial 2 finished with value: 0.8910600498917838 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 2}. Best is trial 2 with value: 0.8910600498917838.\n",
      "[I 2024-12-16 19:52:50,731] Trial 3 finished with value: 0.8909729657057547 and parameters: {'criterion': 'gini', 'min_samples_leaf': 6}. Best is trial 2 with value: 0.8910600498917838.\n",
      "[I 2024-12-16 19:53:06,163] Trial 4 finished with value: 0.8913386497379181 and parameters: {'criterion': 'gini', 'min_samples_leaf': 7}. Best is trial 4 with value: 0.8913386497379181.\n",
      "[I 2024-12-16 19:53:22,273] Trial 5 finished with value: 0.8936999719699856 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 5}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:53:38,587] Trial 6 finished with value: 0.8933695028164761 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 4}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:53:54,208] Trial 7 finished with value: 0.8933175269687214 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 7}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:54:13,554] Trial 8 finished with value: 0.8870155704929071 and parameters: {'criterion': 'gini', 'min_samples_leaf': 1}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:54:29,779] Trial 9 finished with value: 0.8923524618006766 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 3}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:54:45,867] Trial 10 finished with value: 0.8936999719699856 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 5}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:55:01,555] Trial 11 finished with value: 0.8936999719699856 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 5}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:55:17,081] Trial 12 finished with value: 0.8936999719699856 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 5}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:55:33,003] Trial 13 finished with value: 0.8936999719699856 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 5}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:55:49,022] Trial 14 finished with value: 0.8933695028164761 and parameters: {'criterion': 'log_loss', 'min_samples_leaf': 4}. Best is trial 5 with value: 0.8936999719699856.\n",
      "[I 2024-12-16 19:56:04,722] Trial 15 finished with value: 0.8939214637694358 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 6}. Best is trial 15 with value: 0.8939214637694358.\n",
      "[I 2024-12-16 19:56:20,351] Trial 16 finished with value: 0.8939214637694358 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 6}. Best is trial 15 with value: 0.8939214637694358.\n",
      "[I 2024-12-16 19:56:35,746] Trial 17 finished with value: 0.8939214637694358 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 6}. Best is trial 15 with value: 0.8939214637694358.\n",
      "[I 2024-12-16 19:56:51,272] Trial 18 finished with value: 0.8939214637694358 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 6}. Best is trial 15 with value: 0.8939214637694358.\n",
      "[I 2024-12-16 19:57:06,919] Trial 19 finished with value: 0.8933175269687214 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 7}. Best is trial 15 with value: 0.8939214637694358.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 7)\n",
    "    model = (\n",
    "        estimator=DecisionTreeClassifier(\n",
    "            random_state=42, criterion=criterion, min_samples_leaf=min_samples_leaf\n",
    "        ),\n",
    "        n_estimators=20,\n",
    "        random_state=42,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train_downsample,\n",
    "        y_train_downsample,\n",
    "        cv=4,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    roc_auc = score.mean()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study_bagging = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "study_bagging.optimize(objective, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : {'criterion': 'entropy', 'min_samples_leaf': 6}\n",
      "  roc auc: 0.8939214637694358\n"
     ]
    }
   ],
   "source": [
    "print(f\" : {study_bagging.best_params}\")\n",
    "print(f\"  roc auc: {study_bagging.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"estimator__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"estimator__min_samples_leaf\": [1, 3, 5, 7],\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_random_forest = GridSearchCV(\n",
    "#     RandomForestClassifier(\n",
    "#         estimator=DecisionTreeClassifier(random_state=42),\n",
    "#         n_estimators=200,\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1,\n",
    "#     ),\n",
    "#     search_space,\n",
    "#     scoring=\"roc_auc\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=10,\n",
    "#     cv=4,\n",
    "# )\n",
    "# grid_search_random_forest.fit(X_train_downsample, y_train_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Best parameters: {grid_search_random_forest.best_params_}\")\n",
    "# print(f\"Best roc_auc score: {grid_search_random_forest.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:10:31,720] A new study created in memory with name: no-name-d18135e6-7bba-4538-af60-46d95f3b0d6d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1efafe8b14457ab82ec24e16967f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:10:35,711] Trial 0 finished with value: 0.8815049248373457 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 0 with value: 0.8815049248373457.\n",
      "[I 2024-12-16 20:10:57,398] Trial 1 finished with value: 0.8979296564224278 and parameters: {'criterion': 'gini', 'min_samples_leaf': 8, 'max_features': 0.3}. Best is trial 1 with value: 0.8979296564224278.\n",
      "[I 2024-12-16 20:11:24,487] Trial 2 finished with value: 0.9009459811506257 and parameters: {'criterion': 'gini', 'min_samples_leaf': 1, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:11:28,491] Trial 3 finished with value: 0.8962943407486367 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:11:34,303] Trial 4 finished with value: 0.8931836814290536 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:11:59,729] Trial 5 finished with value: 0.9003275649891239 and parameters: {'criterion': 'gini', 'min_samples_leaf': 2, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:12:05,748] Trial 6 finished with value: 0.8944398195184966 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:12:11,836] Trial 7 finished with value: 0.8986244113748304 and parameters: {'criterion': 'gini', 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:12:17,570] Trial 8 finished with value: 0.8954144000079373 and parameters: {'criterion': 'gini', 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:12:23,294] Trial 9 finished with value: 0.8916386812184658 and parameters: {'criterion': 'entropy', 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:12:46,205] Trial 10 finished with value: 0.8995738148881536 and parameters: {'criterion': 'gini', 'min_samples_leaf': 5, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:13:13,318] Trial 11 finished with value: 0.9009459811506257 and parameters: {'criterion': 'gini', 'min_samples_leaf': 1, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:13:40,151] Trial 12 finished with value: 0.9009459811506257 and parameters: {'criterion': 'gini', 'min_samples_leaf': 1, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:14:03,708] Trial 13 finished with value: 0.9000773623339973 and parameters: {'criterion': 'gini', 'min_samples_leaf': 4, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:14:29,049] Trial 14 finished with value: 0.9003279733801073 and parameters: {'criterion': 'gini', 'min_samples_leaf': 2, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:14:56,277] Trial 15 finished with value: 0.9009459811506257 and parameters: {'criterion': 'gini', 'min_samples_leaf': 1, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:14:59,733] Trial 16 finished with value: 0.8927196806503251 and parameters: {'criterion': 'gini', 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:15:22,262] Trial 17 finished with value: 0.8988781120271185 and parameters: {'criterion': 'gini', 'min_samples_leaf': 6, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:15:49,404] Trial 18 finished with value: 0.9009459811506257 and parameters: {'criterion': 'gini', 'min_samples_leaf': 1, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n",
      "[I 2024-12-16 20:16:13,060] Trial 19 finished with value: 0.9000773676380404 and parameters: {'criterion': 'gini', 'min_samples_leaf': 4, 'max_features': 0.3}. Best is trial 2 with value: 0.9009459811506257.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.3])\n",
    "    model = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        criterion=criterion,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        n_estimators=100,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "\n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train_downsample,\n",
    "        y_train_downsample,\n",
    "        cv=4,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    roc_auc = score.mean()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study_rf = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "study_rf.optimize(objective, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : {'criterion': 'gini', 'min_samples_leaf': 1, 'max_features': 0.3}\n",
      "  roc auc: 0.9009459811506257\n"
     ]
    }
   ],
   "source": [
    "print(f\" : {study_rf.best_params}\")\n",
    "print(f\"  roc auc: {study_rf.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"learning_rate\": [0.01, 0.1, 1],\n",
    "#     \"max_depth\": [2, 3],\n",
    "#     \"subsample\": [0.5, 0.8, 1],\n",
    "#     \"max_features\": [\"sqrt\", \"log2\"],\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=GradientBoostingClassifier(random_state=0, n_estimators=300),\n",
    "#     search_space,\n",
    "#     scoring=\"roc_auc\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=10,\n",
    "#     cv=4,\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:23:25,849] A new study created in memory with name: no-name-4f721853-c42e-48d7-9e00-0054496ade6d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b7b762a56f4b5eaeeb73478ae4db68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:23:28,672] Trial 0 finished with value: 0.8760974907439525 and parameters: {'learning_rate': 0.5869779514319597, 'subsample': 0.8616980421755899, 'max_depth': 3, 'max_features': 'log2'}. Best is trial 0 with value: 0.8760974907439525.\n",
      "[I 2024-12-16 20:23:41,800] Trial 1 finished with value: 0.8585727180633632 and parameters: {'learning_rate': 0.8179409796551661, 'subsample': 0.9285055814046063, 'max_depth': 11, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.8760974907439525.\n",
      "[I 2024-12-16 20:23:45,752] Trial 2 finished with value: 0.8495554779528041 and parameters: {'learning_rate': 0.013395137271961084, 'subsample': 0.9132905574090353, 'max_depth': 4, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.8760974907439525.\n",
      "[I 2024-12-16 20:23:54,968] Trial 3 finished with value: 0.897098063599209 and parameters: {'learning_rate': 0.03439208335247732, 'subsample': 0.8870261625146236, 'max_depth': 11, 'max_features': 'log2'}. Best is trial 3 with value: 0.897098063599209.\n",
      "[I 2024-12-16 20:23:56,471] Trial 4 finished with value: 0.86975913301115 and parameters: {'learning_rate': 0.6105133238532833, 'subsample': 0.8962188089335976, 'max_depth': 2, 'max_features': 'log2'}. Best is trial 3 with value: 0.897098063599209.\n",
      "[I 2024-12-16 20:23:59,769] Trial 5 finished with value: 0.8778324520139487 and parameters: {'learning_rate': 0.18838128146293398, 'subsample': 0.978881592948567, 'max_depth': 3, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.897098063599209.\n",
      "[I 2024-12-16 20:24:01,796] Trial 6 finished with value: 0.869906717835223 and parameters: {'learning_rate': 0.777693163533426, 'subsample': 0.9931203737036752, 'max_depth': 3, 'max_features': 'log2'}. Best is trial 3 with value: 0.897098063599209.\n",
      "[I 2024-12-16 20:24:07,270] Trial 7 finished with value: 0.8903208995319555 and parameters: {'learning_rate': 0.32850926844024014, 'subsample': 0.8699569584276134, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.897098063599209.\n",
      "[I 2024-12-16 20:24:11,897] Trial 8 finished with value: 0.8926595188676821 and parameters: {'learning_rate': 0.16090841633288927, 'subsample': 0.8736187204559568, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.897098063599209.\n",
      "[I 2024-12-16 20:24:25,103] Trial 9 finished with value: 0.9091746647943355 and parameters: {'learning_rate': 0.06661758418180654, 'subsample': 0.967070863796536, 'max_depth': 11, 'max_features': 'sqrt'}. Best is trial 9 with value: 0.9091746647943355.\n",
      "[I 2024-12-16 20:24:57,774] Trial 10 finished with value: 0.8973793466209159 and parameters: {'learning_rate': 0.3971773578882349, 'subsample': 0.960781710648923, 'max_depth': 9, 'max_features': 0.3}. Best is trial 9 with value: 0.9091746647943355.\n",
      "[I 2024-12-16 20:25:29,983] Trial 11 finished with value: 0.9007460844923769 and parameters: {'learning_rate': 0.39335020278439153, 'subsample': 0.9607797087527451, 'max_depth': 9, 'max_features': 0.3}. Best is trial 9 with value: 0.9091746647943355.\n",
      "[I 2024-12-16 20:25:58,204] Trial 12 finished with value: 0.8607578095711722 and parameters: {'learning_rate': 0.9909223017876487, 'subsample': 0.9496484709930125, 'max_depth': 8, 'max_features': 0.3}. Best is trial 9 with value: 0.9091746647943355.\n",
      "[I 2024-12-16 20:26:41,814] Trial 13 finished with value: 0.9071663453114843 and parameters: {'learning_rate': 0.29225363280435157, 'subsample': 0.9499586999052876, 'max_depth': 12, 'max_features': 0.3}. Best is trial 9 with value: 0.9091746647943355.\n",
      "[I 2024-12-16 20:27:24,208] Trial 14 finished with value: 0.9110638994897562 and parameters: {'learning_rate': 0.22155399472041465, 'subsample': 0.9309220241259543, 'max_depth': 12, 'max_features': 0.3}. Best is trial 14 with value: 0.9110638994897562.\n",
      "[I 2024-12-16 20:28:06,297] Trial 15 finished with value: 0.9118845795893811 and parameters: {'learning_rate': 0.16369665856162183, 'subsample': 0.9317872320006038, 'max_depth': 12, 'max_features': 0.3}. Best is trial 15 with value: 0.9118845795893811.\n",
      "[I 2024-12-16 20:28:48,139] Trial 16 finished with value: 0.9099676099603426 and parameters: {'learning_rate': 0.2050361662843685, 'subsample': 0.9294227178612855, 'max_depth': 12, 'max_features': 0.3}. Best is trial 15 with value: 0.9118845795893811.\n",
      "[I 2024-12-16 20:29:21,655] Trial 17 finished with value: 0.8936260936456302 and parameters: {'learning_rate': 0.5026368512896685, 'subsample': 0.9101724949095517, 'max_depth': 10, 'max_features': 0.3}. Best is trial 15 with value: 0.9118845795893811.\n",
      "[I 2024-12-16 20:29:49,483] Trial 18 finished with value: 0.9090544381396256 and parameters: {'learning_rate': 0.1433514169663949, 'subsample': 0.9358239204142992, 'max_depth': 8, 'max_features': 0.3}. Best is trial 15 with value: 0.9118845795893811.\n",
      "[I 2024-12-16 20:30:13,775] Trial 19 finished with value: 0.9008435911009913 and parameters: {'learning_rate': 0.279517971366687, 'subsample': 0.9112917958638712, 'max_depth': 7, 'max_features': 0.3}. Best is trial 15 with value: 0.9118845795893811.\n",
      "[I 2024-12-16 20:30:46,434] Trial 20 finished with value: 0.8951813763324012 and parameters: {'learning_rate': 0.494691255238435, 'subsample': 0.8939179182881098, 'max_depth': 10, 'max_features': 0.3}. Best is trial 15 with value: 0.9118845795893811.\n",
      "[I 2024-12-16 20:31:28,348] Trial 21 finished with value: 0.9097647805740805 and parameters: {'learning_rate': 0.2273196298017735, 'subsample': 0.9283225164779315, 'max_depth': 12, 'max_features': 0.3}. Best is trial 15 with value: 0.9118845795893811.\n",
      "[I 2024-12-16 20:32:11,333] Trial 22 finished with value: 0.9134736813252371 and parameters: {'learning_rate': 0.08653468565648717, 'subsample': 0.9415380567591658, 'max_depth': 12, 'max_features': 0.3}. Best is trial 22 with value: 0.9134736813252371.\n",
      "[I 2024-12-16 20:32:46,104] Trial 23 finished with value: 0.9134925483195858 and parameters: {'learning_rate': 0.13388317604283095, 'subsample': 0.943018663702349, 'max_depth': 10, 'max_features': 0.3}. Best is trial 23 with value: 0.9134925483195858.\n",
      "[I 2024-12-16 20:33:20,530] Trial 24 finished with value: 0.9124037511928678 and parameters: {'learning_rate': 0.10450962312036473, 'subsample': 0.945342029290531, 'max_depth': 10, 'max_features': 0.3}. Best is trial 23 with value: 0.9134925483195858.\n",
      "[I 2024-12-16 20:33:54,357] Trial 25 finished with value: 0.8955242526273632 and parameters: {'learning_rate': 0.016095732562481452, 'subsample': 0.9415763416450277, 'max_depth': 10, 'max_features': 0.3}. Best is trial 23 with value: 0.9134925483195858.\n",
      "[I 2024-12-16 20:34:26,786] Trial 26 finished with value: 0.9136812893972152 and parameters: {'learning_rate': 0.10013779492764528, 'subsample': 0.9734974900131835, 'max_depth': 9, 'max_features': 0.3}. Best is trial 26 with value: 0.9136812893972152.\n",
      "[I 2024-12-16 20:34:59,438] Trial 27 finished with value: 0.9113268265181134 and parameters: {'learning_rate': 0.10082653237385297, 'subsample': 0.9833267918678741, 'max_depth': 9, 'max_features': 0.3}. Best is trial 26 with value: 0.9136812893972152.\n",
      "[I 2024-12-16 20:35:29,473] Trial 28 finished with value: 0.8986921551316369 and parameters: {'learning_rate': 0.3495011941648107, 'subsample': 0.9992511417050567, 'max_depth': 8, 'max_features': 0.3}. Best is trial 26 with value: 0.9136812893972152.\n",
      "[I 2024-12-16 20:35:33,629] Trial 29 finished with value: 0.8910095159204388 and parameters: {'learning_rate': 0.08377301097004936, 'subsample': 0.9758482918462704, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 26 with value: 0.9136812893972152.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 1)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.85, 1)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 12)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.3])\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        random_state=0,\n",
    "        n_estimators=100,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "    )\n",
    "\n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train_downsample,\n",
    "        y_train_downsample,\n",
    "        cv=4,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    roc_auc = score.mean()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study_gb = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "study_gb.optimize(objective, n_trials=30, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : {'learning_rate': 0.10013779492764528, 'subsample': 0.9734974900131835, 'max_depth': 9, 'max_features': 0.3}\n",
      "  roc auc: 0.9136812893972152\n"
     ]
    }
   ],
   "source": [
    "print(f\" : {study_gb.best_params}\")\n",
    "print(f\"  roc auc: {study_gb.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:35:33,643] A new study created in memory with name: no-name-6715746a-b1c4-4d06-be96-6cce2052de4b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1857ccb7828147fbbc6b95518b95297d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:35:44,080] Trial 0 finished with value: 0.843586807867792 and parameters: {'learning_rate': 0.9066489073674951, 'subsample': 0.8590914231977526, 'max_depth': 6, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.843586807867792.\n",
      "[I 2024-12-16 20:35:52,034] Trial 1 finished with value: 0.8852800284114677 and parameters: {'learning_rate': 0.023638630885344365, 'subsample': 0.9111565914771669, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 1 with value: 0.8852800284114677.\n",
      "[I 2024-12-16 20:35:58,227] Trial 2 finished with value: 0.8521705672389777 and parameters: {'learning_rate': 0.016198260686895616, 'subsample': 0.9466561965478197, 'max_depth': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.8852800284114677.\n",
      "[I 2024-12-16 20:36:01,969] Trial 3 finished with value: 0.8768112978620561 and parameters: {'learning_rate': 0.6317929491976992, 'subsample': 0.9347392194269208, 'max_depth': 3, 'max_features': 'log2'}. Best is trial 1 with value: 0.8852800284114677.\n",
      "[I 2024-12-16 20:36:48,402] Trial 4 finished with value: 0.8837006811047264 and parameters: {'learning_rate': 0.5404179878887908, 'subsample': 0.8622539871526356, 'max_depth': 7, 'max_features': 0.3}. Best is trial 1 with value: 0.8852800284114677.\n",
      "[I 2024-12-16 20:36:59,634] Trial 5 finished with value: 0.8931052445177855 and parameters: {'learning_rate': 0.39193910039562313, 'subsample': 0.9278294801507081, 'max_depth': 9, 'max_features': 'log2'}. Best is trial 5 with value: 0.8931052445177855.\n",
      "[I 2024-12-16 20:37:54,825] Trial 6 finished with value: 0.9120342109527475 and parameters: {'learning_rate': 0.1698615007120068, 'subsample': 0.9214126594116056, 'max_depth': 8, 'max_features': 0.3}. Best is trial 6 with value: 0.9120342109527475.\n",
      "[I 2024-12-16 20:38:36,522] Trial 7 finished with value: 0.8792595996397242 and parameters: {'learning_rate': 0.01137561832632352, 'subsample': 0.9525425063419115, 'max_depth': 6, 'max_features': 0.3}. Best is trial 6 with value: 0.9120342109527475.\n",
      "[I 2024-12-16 20:38:45,790] Trial 8 finished with value: 0.8961179992392552 and parameters: {'learning_rate': 0.2304128850442866, 'subsample': 0.9001149889103538, 'max_depth': 5, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.9120342109527475.\n",
      "[I 2024-12-16 20:38:53,717] Trial 9 finished with value: 0.8876039315538786 and parameters: {'learning_rate': 0.3942683579665371, 'subsample': 0.8930945690000858, 'max_depth': 4, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.9120342109527475.\n",
      "[I 2024-12-16 20:40:23,188] Trial 10 finished with value: 0.9126469490845629 and parameters: {'learning_rate': 0.25903758826283496, 'subsample': 0.9980269168580302, 'max_depth': 12, 'max_features': 0.3}. Best is trial 10 with value: 0.9126469490845629.\n",
      "[I 2024-12-16 20:41:53,005] Trial 11 finished with value: 0.9133175321717222 and parameters: {'learning_rate': 0.2303298038104793, 'subsample': 0.9908986137592783, 'max_depth': 12, 'max_features': 0.3}. Best is trial 11 with value: 0.9133175321717222.\n",
      "[I 2024-12-16 20:43:22,000] Trial 12 finished with value: 0.9127588777971234 and parameters: {'learning_rate': 0.2714068742356181, 'subsample': 0.9966920382633783, 'max_depth': 12, 'max_features': 0.3}. Best is trial 11 with value: 0.9133175321717222.\n",
      "[I 2024-12-16 20:44:50,677] Trial 13 finished with value: 0.9076875756282705 and parameters: {'learning_rate': 0.7333866055812039, 'subsample': 0.9987194549543728, 'max_depth': 12, 'max_features': 0.3}. Best is trial 11 with value: 0.9133175321717222.\n",
      "[I 2024-12-16 20:46:03,104] Trial 14 finished with value: 0.9083234140942719 and parameters: {'learning_rate': 0.3675320357162539, 'subsample': 0.9742621426598695, 'max_depth': 10, 'max_features': 0.3}. Best is trial 11 with value: 0.9133175321717222.\n",
      "[I 2024-12-16 20:47:22,973] Trial 15 finished with value: 0.9131592353641019 and parameters: {'learning_rate': 0.2030141692891893, 'subsample': 0.9750154724395358, 'max_depth': 11, 'max_features': 0.3}. Best is trial 11 with value: 0.9133175321717222.\n",
      "[I 2024-12-16 20:48:35,230] Trial 16 finished with value: 0.9150611490376279 and parameters: {'learning_rate': 0.14360046631995532, 'subsample': 0.9676374684474207, 'max_depth': 10, 'max_features': 0.3}. Best is trial 16 with value: 0.9150611490376279.\n",
      "[I 2024-12-16 20:49:50,725] Trial 17 finished with value: 0.9146883729046931 and parameters: {'learning_rate': 0.129346590260895, 'subsample': 0.9722257822325001, 'max_depth': 10, 'max_features': 0.3}. Best is trial 16 with value: 0.9150611490376279.\n",
      "[I 2024-12-16 20:51:03,744] Trial 18 finished with value: 0.9156105954421854 and parameters: {'learning_rate': 0.13614341921546924, 'subsample': 0.9627358535570463, 'max_depth': 10, 'max_features': 0.3}. Best is trial 18 with value: 0.9156105954421854.\n",
      "[I 2024-12-16 20:51:15,748] Trial 19 finished with value: 0.8909092793461428 and parameters: {'learning_rate': 0.49236924497991963, 'subsample': 0.9543770686087749, 'max_depth': 9, 'max_features': 'log2'}. Best is trial 18 with value: 0.9156105954421854.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 1)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.85, 1)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 12)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.3])\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        random_state=0,\n",
    "        n_estimators=200,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "    )\n",
    "\n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train_downsample,\n",
    "        y_train_downsample,\n",
    "        cv=4,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    roc_auc = score.mean()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study_gb = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "study_gb.optimize(objective, n_trials=30, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : {'learning_rate': 0.13614341921546924, 'subsample': 0.9627358535570463, 'max_depth': 10, 'max_features': 0.3}\n",
      "  roc auc: 0.9156105954421854\n"
     ]
    }
   ],
   "source": [
    "print(f\" : {study_gb.best_params}\")\n",
    "print(f\"  roc auc: {study_gb.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:51:15,764] A new study created in memory with name: no-name-5f3a311e-2225-4106-8eca-d475d95003d8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2165ff52024124bb0aa9b95af8abb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:51:21,524] Trial 0 finished with value: 0.8866848918519936 and parameters: {'learning_rate': 0.3734301653855022, 'subsample': 0.9346328802187807, 'max_depth': 3, 'max_features': 'log2'}. Best is trial 0 with value: 0.8866848918519936.\n",
      "[I 2024-12-16 20:51:42,797] Trial 1 finished with value: 0.9039044803534889 and parameters: {'learning_rate': 0.2298366389461664, 'subsample': 0.9746625474087359, 'max_depth': 7, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.9039044803534889.\n",
      "[I 2024-12-16 20:53:51,768] Trial 2 finished with value: 0.9176256408487836 and parameters: {'learning_rate': 0.11916998825374048, 'subsample': 0.9304122375880862, 'max_depth': 12, 'max_features': 0.3}. Best is trial 2 with value: 0.9176256408487836.\n",
      "[I 2024-12-16 20:55:06,165] Trial 3 finished with value: 0.9124566564820745 and parameters: {'learning_rate': 0.13489806886681427, 'subsample': 0.9115561837039293, 'max_depth': 7, 'max_features': 0.3}. Best is trial 2 with value: 0.9176256408487836.\n",
      "[I 2024-12-16 20:55:19,833] Trial 4 finished with value: 0.8757836722514961 and parameters: {'learning_rate': 0.7243853249069481, 'subsample': 0.9863357863676754, 'max_depth': 7, 'max_features': 'log2'}. Best is trial 2 with value: 0.9176256408487836.\n",
      "[I 2024-12-16 20:56:07,690] Trial 5 finished with value: 0.8961739296474421 and parameters: {'learning_rate': 0.28274109781705886, 'subsample': 0.9341839218324167, 'max_depth': 4, 'max_features': 0.3}. Best is trial 2 with value: 0.9176256408487836.\n",
      "[I 2024-12-16 20:57:24,696] Trial 6 finished with value: 0.8579998131430544 and parameters: {'learning_rate': 0.9718385445596534, 'subsample': 0.9355190533692078, 'max_depth': 7, 'max_features': 0.3}. Best is trial 2 with value: 0.9176256408487836.\n",
      "[I 2024-12-16 20:57:46,625] Trial 7 finished with value: 0.9011276968924444 and parameters: {'learning_rate': 0.3112780156234442, 'subsample': 0.9212128666680753, 'max_depth': 10, 'max_features': 'log2'}. Best is trial 2 with value: 0.9176256408487836.\n",
      "[I 2024-12-16 20:57:52,164] Trial 8 finished with value: 0.8716698234491113 and parameters: {'learning_rate': 0.6638698669624843, 'subsample': 0.856723546300327, 'max_depth': 3, 'max_features': 'log2'}. Best is trial 2 with value: 0.9176256408487836.\n",
      "[I 2024-12-16 20:58:18,649] Trial 9 finished with value: 0.8926740135582114 and parameters: {'learning_rate': 0.3491375427083619, 'subsample': 0.8918981175013723, 'max_depth': 9, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9176256408487836.\n",
      "[I 2024-12-16 21:00:33,063] Trial 10 finished with value: 0.9188111178272939 and parameters: {'learning_rate': 0.04922484544238698, 'subsample': 0.9635175126291756, 'max_depth': 12, 'max_features': 0.3}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:02:48,077] Trial 11 finished with value: 0.9123756307773172 and parameters: {'learning_rate': 0.012117805015374433, 'subsample': 0.9653972148834118, 'max_depth': 12, 'max_features': 0.3}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:05:02,915] Trial 12 finished with value: 0.9187104394086121 and parameters: {'learning_rate': 0.03649083996067903, 'subsample': 0.9560914152570414, 'max_depth': 12, 'max_features': 0.3}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:06:50,214] Trial 13 finished with value: 0.9031566651328586 and parameters: {'learning_rate': 0.008746176489225634, 'subsample': 0.9575721110503609, 'max_depth': 10, 'max_features': 0.3}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:09:05,975] Trial 14 finished with value: 0.9121059623108827 and parameters: {'learning_rate': 0.5100144735177816, 'subsample': 0.9968091466206803, 'max_depth': 12, 'max_features': 0.3}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:11:04,276] Trial 15 finished with value: 0.9162768259288765 and parameters: {'learning_rate': 0.1427327659869293, 'subsample': 0.954761432697835, 'max_depth': 10, 'max_features': 0.3}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:11:31,754] Trial 16 finished with value: 0.8998808189162572 and parameters: {'learning_rate': 0.5056409396444512, 'subsample': 0.9498672898656764, 'max_depth': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:12:24,279] Trial 17 finished with value: 0.8893041998181659 and parameters: {'learning_rate': 0.02075032511183196, 'subsample': 0.8995483841620764, 'max_depth': 5, 'max_features': 0.3}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:14:30,431] Trial 18 finished with value: 0.9144203083396139 and parameters: {'learning_rate': 0.19775042876731644, 'subsample': 0.9830858600512062, 'max_depth': 11, 'max_features': 0.3}. Best is trial 10 with value: 0.9188111178272939.\n",
      "[I 2024-12-16 21:15:11,264] Trial 19 finished with value: 0.9061994836816949 and parameters: {'learning_rate': 0.41165777864837516, 'subsample': 0.9673743704571687, 'max_depth': 11, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.9188111178272939.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 1)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.85, 1)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 12)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.3])\n",
    "\n",
    "    model = GradientBoostingClassifier(\n",
    "        random_state=0,\n",
    "        n_estimators=300,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "    )\n",
    "\n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train_downsample,\n",
    "        y_train_downsample,\n",
    "        cv=4,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    roc_auc = score.mean()\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "study_gb = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "study_gb.optimize(objective, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : {'learning_rate': 0.04922484544238698, 'subsample': 0.9635175126291756, 'max_depth': 12, 'max_features': 0.3}\n",
      "  roc auc: 0.9188111178272939\n"
     ]
    }
   ],
   "source": [
    "print(f\" : {study_gb.best_params}\")\n",
    "print(f\"  roc auc: {study_gb.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimators = [\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=7, p=1)),\n",
    "    (\n",
    "        \"random_forest\",\n",
    "        RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            criterion=\"gini\",\n",
    "            min_samples_leaf=1,\n",
    "            max_features=0.3,\n",
    "            n_estimators=100,\n",
    "            n_jobs=4,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"gradient_boosting\",\n",
    "        GradientBoostingClassifier(\n",
    "            random_state=0,\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.049,\n",
    "            subsample=0.963,\n",
    "            max_depth=12,\n",
    "            max_features=0.3,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators, final_estimator=GradientBoostingClassifier(), n_jobs=8\n",
    ")\n",
    "# stacking_clf.fit(X_train_downsample, y_train_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(\n",
    "    stacking_clf,\n",
    "    X_train_downsample,\n",
    "    y_train_downsample,\n",
    "    cv=4,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=8,\n",
    ")\n",
    "roc_auc = score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9198285185079728"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc of stacking: 0.9153554342760114\n"
     ]
    }
   ],
   "source": [
    "# print(f\"roc auc of stacking: {roc_auc_score(y_val_downsample, stacking_clf.predict_proba(X_val_downsample)[:,1])}\")\n",
    "# roc auc of stacking: 0.9153554342760114\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_val, knn_on_all.predict_proba(X_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
